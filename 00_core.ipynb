{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1629d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae282a",
   "metadata": {},
   "source": [
    "# Gaspard's source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116139",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, typing, mimetypes, base64, json, ast, io, os, time, proto\n",
    "import filetype as ft\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types.generation_types import GenerateContentResponse, GenerationConfig\n",
    "from google.generativeai.protos import FunctionCall, Content, FunctionResponse, FunctionDeclaration\n",
    "from google.generativeai.protos import GenerateContentResponse as GCR\n",
    "from google.generativeai import protos\n",
    "\n",
    "import toolslm\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "\n",
    "from collections import abc\n",
    "\n",
    "from proto.marshal.collections.maps import MapComposite\n",
    "from proto.marshal.collections.repeated import RepeatedComposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0764659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc\n",
    "\n",
    "try: from IPython import display\n",
    "except: display=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "UsageMetadata = GCR.UsageMetadata\n",
    "empty = inspect.Parameter.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "models = 'gemini-1.5-pro-exp-0827', 'gemini-1.5-flash-exp-0827','gemini-1.5-pro','gemini-1.5-flash'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a095bc",
   "metadata": {},
   "source": [
    "These are the latest version of Gemini models available at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c81236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99b6d9",
   "metadata": {},
   "source": [
    "We'll use gemini-1.5-flash for the examples since it's faster and cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba835d",
   "metadata": {},
   "source": [
    "## Gemini SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a343d78",
   "metadata": {},
   "source": [
    "Follow the [instructions](https://aistudio.google.com/app/apikey) to generate an API key, and set it as an evironment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = genai.GenerativeModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62f521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi Faisal! It's nice to meet you. What can I do for you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 19,\n",
       "        \"total_token_count\": 27\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.generate_content(\"Hi, I'm Faisal!\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae6c0b",
   "metadata": {},
   "source": [
    "## Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97739d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_block(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Find the content in `r`.\"\n",
    "    m = nested_idx(r, 'candidates', 0)\n",
    "    if not m: return m\n",
    "    if hasattr(m, 'content'): return m.content \n",
    "    else: return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b4144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  text: \"Hi Faisal! It\\'s nice to meet you. What can I do for you today? \\n\"\n",
       "}\n",
       "role: \"model\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_block(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    if hasattr(blk, 'parts'): return getattr(blk,'parts')[0].text\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d7340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Faisal! It's nice to meet you. What can I do for you today? \\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch()\n",
    "def _repr_markdown_(self:GenerateContentResponse):\n",
    "    met = list(self.to_dict()['candidates'][0].items()) + list(self.to_dict()['usage_metadata'].items())\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in met)\n",
    "    res = contents(self)\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{contents(self)}\\n<details>\\n\\n- {det}\\n\\n</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Faisal! It's nice to meet you. What can I do for you today? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"Hi Faisal! It's nice to meet you. What can I do for you today? \\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 8\n",
       "- candidates_token_count: 19\n",
       "- total_token_count: 27\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi Faisal! It's nice to meet you. What can I do for you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 19,\n",
       "        \"total_token_count\": 27\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 8\n",
       "candidates_token_count: 19\n",
       "total_token_count: 27"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61918cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of input tokens\n",
    "          out=0  # Number of output tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `Usage`.\"\n",
    "    return UsageMetadata(prompt_token_count=inp, candidates_token_count=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443d1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def total(self:UsageMetadata): return self.prompt_token_count+self.candidates_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:UsageMetadata): return f'In: {self.prompt_token_count}; Out: {self.candidates_token_count}; Total: {self.total}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6780c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 19; Total: 27"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fafcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:UsageMetadata, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_token_count+b.prompt_token_count, self.candidates_token_count+b.candidates_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 16; Out: 38; Total: 54"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage_metadata+r.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bc41b",
   "metadata": {},
   "source": [
    "## Creating messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse):\n",
    "        blk = find_block(content)\n",
    "        role = blk.role\n",
    "        content = blk.parts[0].text\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924dece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'parts': [\"I'm Faisal\"]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Faisal\"\n",
    "m = mk_msg(prompt)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Faisal! What can I do for you today? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Nice to meet you, Faisal! What can I do for you today? \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 5\n",
       "- candidates_token_count: 16\n",
       "- total_token_count: 21\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Nice to meet you, Faisal! What can I do for you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 5,\n",
       "        \"candidates_token_count\": 16,\n",
       "        \"total_token_count\": 21\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.generate_content([m], generation_config=GenerationConfig(max_output_tokens=100))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ede3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [\"I'm Faisal\"]},\n",
       " {'role': 'model',\n",
       "  'parts': ['Nice to meet you, Faisal! What can I do for you today? \\n']},\n",
       " {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [mk_msg(prompt), mk_msg(r), mk_msg('I forgot my name. Can you remind me please?')]\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708fc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You just told me your name is Faisal! 😊 \n",
       "\n",
       "Do you need help remembering other things, or is this a joke? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'You just told me your name is Faisal! 😊 \\n\\nDo you need help remembering other things, or is this a joke? \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 35\n",
       "- candidates_token_count: 26\n",
       "- total_token_count: 61\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"You just told me your name is Faisal! \\ud83d\\ude0a \\n\\nDo you need help remembering other things, or is this a joke? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 35,\n",
       "        \"candidates_token_count\": 26,\n",
       "        \"total_token_count\": 61\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a573d",
   "metadata": {},
   "source": [
    "Let's make this a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08352a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27ee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [\"Hi, I'm Faisal!\"]},\n",
       " {'role': 'model',\n",
       "  'parts': ['Nice to meet you, Faisal! What can I do for you today? \\n']},\n",
       " {'role': 'user', 'parts': ['I forgot my name. Can you remind me please?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([\"Hi, I'm Faisal!\", r, \"I forgot my name. Can you remind me please?\"]); msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcb0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You're Faisal! 😊  I'm happy to help. \n",
       "\n",
       "Is there anything else I can assist you with? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"You're Faisal! 😊  I'm happy to help. \\n\\nIs there anything else I can assist you with? \\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 38\n",
       "- candidates_token_count: 26\n",
       "- total_token_count: 64\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"You're Faisal! \\ud83d\\ude0a  I'm happy to help. \\n\\nIs there anything else I can assist you with? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 38,\n",
       "        \"candidates_token_count\": 26,\n",
       "        \"total_token_count\": 64\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.generate_content(msgs, generation_config=GenerationConfig(max_output_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec007f7f",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05864cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None, sp=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.sp = sp\n",
    "        self.c = (cli or genai.GenerativeModel(model, system_instruction=sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db82494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r:GenerateContentResponse):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage_metadata',None): self.use += r.usage_metadata\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 16; Total: 21"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stream(r):\n",
    "    for o in r:\n",
    "        o = contents(o)\n",
    "        if o and isinstance(o, str): yield(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _set_sp(self:Client, sp:str):\n",
    "    if sp != self.sp:\n",
    "        self.sp = sp\n",
    "        self.c = genai.GenerativeModel(model, system_instruction=self.sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5166bd",
   "metadata": {},
   "source": [
    "Gemini cli requires passing the system prompt when creating the client, so we recreate the client for now.\n",
    "\n",
    "TODO: Ask Google to surface this option to generate_content function, since they're passing the system prompt to each request anyways [under the hood](https://github.com/googleapis/python-aiplatform/blob/f89df1f30822d260176487f74c3743cab88a38fd/vertexai/generative_models/_generative_models.py#L446)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba14a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _precall(self:Client, msgs):\n",
    "    if not isinstance(msgs,list): msgs = [msgs]\n",
    "    msgs = mk_msgs(msgs)\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bac993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(genai.GenerativeModel.generate_content)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str=None, # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if sp: self._set_sp(sp)\n",
    "    msgs = self._precall(msgs)\n",
    "    gc_params = inspect.signature(GenerationConfig.__init__).parameters\n",
    "    gc_kwargs = {k: v for k, v in kwargs.items() if k in gc_params}\n",
    "    gen_config = GenerationConfig(max_output_tokens=maxtok, **gc_kwargs)\n",
    "    gen_params = inspect.signature(self.c.generate_content).parameters\n",
    "    gen_kwargs = {k: v for k, v in kwargs.items() if k in gen_params}\n",
    "    r = self.c.generate_content(\n",
    "        contents=msgs, generation_config=gen_config, stream=stream, **gen_kwargs)\n",
    "    if not stream: return self._r(r)\n",
    "    else: return get_stream(map(self._r, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today? \\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.c.generate_content('hi').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50775d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi there! How can I help you today? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Hi there! How can I help you today? \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 2\n",
       "- candidates_token_count: 10\n",
       "- total_token_count: 12\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hi there! How can I help you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 2,\n",
       "        \"candidates_token_count\": 10,\n",
       "        \"total_token_count\": 12\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 7; Out: 26; Total: 33"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97727a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there!  How can I help you today? \n"
     ]
    }
   ],
   "source": [
    "for o in c(msgs, stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 11; Out: 40; Total: 51"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c36641",
   "metadata": {},
   "source": [
    "Gemini cli requires passing the system prompt when creating the client, but we didn't pass one at creation time.\n",
    "Let's make sure that it gets set properly when we call the client later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"Respond only in emojis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a978d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "👋 \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': '👋 \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 6\n",
       "- candidates_token_count: 1\n",
       "- total_token_count: 7\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"\\ud83d\\udc4b \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 6,\n",
       "        \"candidates_token_count\": 1,\n",
       "        \"total_token_count\": 7\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs, sp=sysp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f168bb",
   "metadata": {},
   "source": [
    "We've shown the token usage but we really care about is pricing. Let's extract the latest pricing from Google into a pricing dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "pricing = {  # model type: $ / million tokens (input, output, cache, input_long, output_long, cache_long)\n",
    "    'gemini-1.5-pro': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n",
    "    'gemini-1.5-pro-exp-0827': (1.25, 5.0, 0.3125, 2.50, 10.0, 0.625),\n",
    "    'gemini-1.5-flash': (0.075, 0.30, 0.01875, 0.15, 0.60, 0.0375),\n",
    "    'gemini-1.5-flash-exp-0827': (0.075, 0.30, 0.01875, 0.15, 0.60, 0.0375)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab805798",
   "metadata": {},
   "source": [
    "Now let's add a cost prop to `Client` to calculate the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_pricing(m, u):\n",
    "    return pricing[m][:3] if u.prompt_token_count < 128_000 else pricing[m][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self:Client):\n",
    "    inp_cost, out_cost, cache_cost = get_pricing(self.model.split('-exp-')[0], self.use)\n",
    "    return (self.use.prompt_token_count * inp_cost + self.use.candidates_token_count * out_cost + self.use.cached_content_token_count * cache_cost) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a48912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3575e-05"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _repr_markdown_(self:Client):\n",
    "    if not hasattr(self,'result'): return 'No results yet'\n",
    "    msg = contents(self.result)\n",
    "    inp_cost,out_cost,_ = get_pricing(self.model.split('-exp-')[0], self.use)\n",
    "    in_cost = self.use.prompt_token_count * inp_cost/1e6\n",
    "    out_cost = self.use.candidates_token_count * out_cost/1e6\n",
    "    cache_cost = self.use.cached_content_token_count * out_cost/1e6\n",
    "    return f\"\"\"{msg}\n",
    "\n",
    "| Metric | Count | Cost (USD) |\n",
    "|--------|------:|-----:|\n",
    "| Input tokens | {self.use.prompt_token_count:,} | {in_cost:.6f} |\n",
    "| Output tokens | {self.use.candidates_token_count:,} | {out_cost:.6f} |\n",
    "| Cache tokens | {self.use.cached_content_token_count:,} | {cache_cost:.6f} |\n",
    "| **Total** | **{self.use.total:,}** | **${self.cost:.6f}** |\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8304456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "👋 \n",
       "\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 17 | 0.000001 |\n",
       "| Output tokens | 41 | 0.000012 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **58** | **$0.000014** |"
      ],
      "text/plain": [
       "<__main__.Client>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dee9a",
   "metadata": {},
   "source": [
    "## Tool Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ec83c",
   "metadata": {},
   "source": [
    "Google's Genai API handles schema exatraction under the hood, so we can just directly pass the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f714d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 86\n",
       "- candidates_token_count: 29\n",
       "- total_token_count: 115\n",
       "- cached_content_token_count: 0"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 86,\n",
       "        \"candidates_token_count\": 29,\n",
       "        \"total_token_count\": 115\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = c(pr, sp=sysp, tools=[sums])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ea030",
   "metadata": {},
   "source": [
    "Looks like our output isn't pretty anymore. Let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    \n",
    "    if hasattr(blk, 'parts'):\n",
    "        part = blk.parts[0]\n",
    "        if 'text' in part:\n",
    "            return part.text\n",
    "        else:\n",
    "            return part\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a9585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 86\n",
       "- candidates_token_count: 29\n",
       "- total_token_count: 115\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 86,\n",
       "        \"candidates_token_count\": 29,\n",
       "        \"total_token_count\": 115\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163e293",
   "metadata": {},
   "source": [
    "B-e-a-utiful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441a121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  function_call {\n",
       "    name: \"sums\"\n",
       "    args {\n",
       "      fields {\n",
       "        key: \"b\"\n",
       "        value {\n",
       "          number_value: 6458932\n",
       "        }\n",
       "      }\n",
       "      fields {\n",
       "        key: \"a\"\n",
       "        value {\n",
       "          number_value: 604542\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "role: \"model\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = find_block(r); m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"sums\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"b\"\n",
       "    value {\n",
       "      number_value: 6458932\n",
       "    }\n",
       "  }\n",
       "  fields {\n",
       "    key: \"a\"\n",
       "    value {\n",
       "      number_value: 604542\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = m.parts[0].function_call; func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365966dd",
   "metadata": {},
   "source": [
    "Let's get the returned function call into a format that is expected by `call_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 604542.0\n",
      "b 6458932.0\n"
     ]
    }
   ],
   "source": [
    "for k,v in func.args.items(): print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36851c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_args(args): return {k: v for k,v in args.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb429a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 604542.0, 'b': 6458932.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_args(func.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def convert_func(f): return AttrDict(name=f.name, inputs=mk_args(f.args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977445b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{'inputs': {'a': 604542.0, 'b': 6458932.0}, 'name': 'sums'}\n",
       "```"
      ],
      "text/plain": [
       "{'name': 'sums', 'inputs': {'a': 604542.0, 'b': 6458932.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = convert_func(func); func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9127c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sums': <function __main__.sums(a: int, b: int) -> int>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums); ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d387b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = call_func(func.name, func.inputs, ns); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb08798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n",
    "    if isinstance(content, dict): role,content = content['role'],content['parts']\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Tool use request response\n",
    "    ns, # Namespace to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    parts = find_block(r).parts\n",
    "    tcs = [p.function_call for p in parts if hasattr(p, 'function_call')]\n",
    "    res = [mk_msg(r)]\n",
    "    tc_res = []\n",
    "    for func in (tcs or []):\n",
    "        if not func: continue\n",
    "        func = convert_func(func)\n",
    "        cts = call_func(func.name, func.inputs, ns=ns)\n",
    "        tc_res.append(FunctionResponse(name=func.name, response={'result': cts}))\n",
    "    if tc_res: res.append(mk_msg(tc_res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [name: \"sums\"\n",
       "  response {\n",
       "    fields {\n",
       "      key: \"result\"\n",
       "      value {\n",
       "        number_value: 7063474\n",
       "      }\n",
       "    }\n",
       "  }]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is 604542+6458932?',\n",
       " {'role': 'model', 'parts': [function_call {\n",
       "     name: \"sums\"\n",
       "     args {\n",
       "       fields {\n",
       "         key: \"b\"\n",
       "         value {\n",
       "           number_value: 6458932\n",
       "         }\n",
       "       }\n",
       "       fields {\n",
       "         key: \"a\"\n",
       "         value {\n",
       "           number_value: 604542\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }]},\n",
       " {'role': 'user',\n",
       "  'parts': [name: \"sums\"\n",
       "   response {\n",
       "     fields {\n",
       "       key: \"result\"\n",
       "       value {\n",
       "         number_value: 7063474\n",
       "       }\n",
       "     }\n",
       "   }]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [pr] + tr; msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7848bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': ['What is 604542+6458932?']},\n",
       " {'role': 'model',\n",
       "  'parts': [function_call {\n",
       "     name: \"sums\"\n",
       "     args {\n",
       "       fields {\n",
       "         key: \"b\"\n",
       "         value {\n",
       "           number_value: 6458932\n",
       "         }\n",
       "       }\n",
       "       fields {\n",
       "         key: \"a\"\n",
       "         value {\n",
       "           number_value: 604542\n",
       "         }\n",
       "       }\n",
       "     }\n",
       "   }]},\n",
       " {'role': 'user',\n",
       "  'parts': [name: \"sums\"\n",
       "   response {\n",
       "     fields {\n",
       "       key: \"result\"\n",
       "       value {\n",
       "         number_value: 7063474\n",
       "       }\n",
       "     }\n",
       "   }]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1a15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "7063474 \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': '7063474 \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 136\n",
       "- candidates_token_count: 7\n",
       "- total_token_count: 143\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"7063474 \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 136,\n",
       "        \"candidates_token_count\": 7,\n",
       "        \"total_token_count\": 143\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(msgs, sp=sysp, tools=[sums])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e310c",
   "metadata": {},
   "source": [
    "We can also force a particular set of tools to be used using, `tool_config`. Here's an example of how to do that for genai api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1badc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tool_config(choose: list)->dict:\n",
    "    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\": [x.__name__ for x in choose]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dc557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_calling_config': {'mode': 'ANY',\n",
       "  'allowed_function_names': ['sums']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_config = mk_tool_config([sums]); tool_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba07ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 2\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 1.0, 'b': 2.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 70\n",
       "- candidates_token_count: 18\n",
       "- total_token_count: 88\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 1.0,\n",
       "                    \"b\": 2.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 70,\n",
       "        \"candidates_token_count\": 18,\n",
       "        \"total_token_count\": 88\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c('Howdy!', tools=[sums], tool_config=tool_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50421e42",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787ff76",
   "metadata": {},
   "source": [
    "We can also use tool calling to force the model to return structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs:list, # The prompt or list of prompts\n",
    "               tools:list, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    if not isinstance(tools, list): tools = [tools]\n",
    "    kwargs['tools'] = tools\n",
    "    kwargs['tool_config'] = mk_tool_config(tools)\n",
    "    res = self(msgs, **kwargs)\n",
    "    ns=mk_ns(*tools)\n",
    "    parts = find_block(res).parts\n",
    "    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n",
    "    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15813bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe(BasicRepr):\n",
    "    \"A structure for representing recipes.\"\n",
    "    def __init__(self, recipe_name: str, ingredients: list[str]): store_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6def5",
   "metadata": {},
   "source": [
    "Gemini API schema extraction doesn't work very well for Class definitions so we define a factory method as a workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7083831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(recipe_name='Chocolate Chip Cookies', ingredients=['flour', 'sugar', 'eggs', 'chocolate chips', 'butter', 'vanilla extract'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = \"Give me a receipe for chocolate chip cookies\"\n",
    "recipe = c.structured(pr, tools=[Recipe], sp=sysp)[0]; recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b51ff0",
   "metadata": {},
   "source": [
    "This works great, however, to handle to complex structured output usecases we need to manually create the schema objects to feed to the GenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turn(BasicRepr):\n",
    "    \"Turn in the conversation\"\n",
    "    def __init__(self, msg_a: str, msg_b: str): store_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation(BasicRepr):\n",
    "    \"A conversation between two people\"\n",
    "    def __init__(self, turns: list[Turn]): store_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9b992",
   "metadata": {},
   "source": [
    "Let's do this manually using GenAI's special protobuf schema types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c95a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn = genai.protos.Schema(\n",
    "    type = genai.protos.Type.OBJECT,\n",
    "    properties = {\n",
    "        'msg_a':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "        'msg_b':  genai.protos.Schema(type=genai.protos.Type.STRING),\n",
    "    },\n",
    "    required=['msg_a', 'msg_b']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ca8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = genai.protos.Schema(\n",
    "    type = genai.protos.Type.OBJECT,\n",
    "    properties = {\n",
    "        'turns':  genai.protos.Schema(type=genai.protos.Type.ARRAY, items=turn)\n",
    "    },\n",
    "    required=['turns']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f81ed",
   "metadata": {},
   "source": [
    "Great, now, let's wrap this into the necessary function declaration that will then be used as a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11391714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"create_convo\"\n",
       "description: \"Creates a conversation\"\n",
       "parameters {\n",
       "  type_: OBJECT\n",
       "  properties {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      type_: ARRAY\n",
       "      items {\n",
       "        type_: OBJECT\n",
       "        properties {\n",
       "          key: \"msg_b\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        properties {\n",
       "          key: \"msg_a\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        required: \"msg_a\"\n",
       "        required: \"msg_b\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  required: \"turns\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_convo = genai.protos.FunctionDeclaration(\n",
    "    name=\"create_convo\",\n",
    "    description=\"Creates a conversation\",\n",
    "    parameters=convo\n",
    "); create_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = genai.GenerativeModel(model_name=model, tools = [create_convo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11c4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"create_convo\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"turns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            struct_value {\n",
       "              fields {\n",
       "                key: \"msg_b\"\n",
       "                value {\n",
       "                  string_value: \"\"\n",
       "                }\n",
       "              }\n",
       "              fields {\n",
       "                key: \"msg_a\"\n",
       "                value {\n",
       "                  string_value: \"I\\'d love to give you a recipe for chocolate chip cookies, but I don\\'t have access to the internet to get one. Can I help you with something else?\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'create_convo', 'args': {'turns': [{'msg_b': '', 'msg_a': \"I'd love to give you a recipe for chocolate chip cookies, but I don't have access to the internet to get one. Can I help you with something else?\"}]}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 109\n",
       "- candidates_token_count: 64\n",
       "- total_token_count: 173\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"create_convo\",\n",
       "                  \"args\": {\n",
       "                    \"turns\": [\n",
       "                      {\n",
       "                        \"msg_b\": \"\",\n",
       "                        \"msg_a\": \"I'd love to give you a recipe for chocolate chip cookies, but I don't have access to the internet to get one. Can I help you with something else?\"\n",
       "                      }\n",
       "                    ]\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 109,\n",
       "        \"candidates_token_count\": 64,\n",
       "        \"total_token_count\": 173\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1cf965",
   "metadata": {},
   "source": [
    "Great, now let's start by taking our normal JSON schema that we get from the helper function `get_schema` and converting it to a protobuf schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "j2p_map = {\n",
    "    'string': protos.Type.STRING,\n",
    "    'array': protos.Type.ARRAY,\n",
    "    'object': protos.Type.OBJECT,\n",
    "    'integer': protos.Type.INTEGER,\n",
    "    'number': protos.Type.NUMBER,\n",
    "    'boolean': protos.Type.BOOLEAN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00344ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Conversation',\n",
       " 'description': 'A conversation between two people',\n",
       " 'input_schema': {'type': 'object',\n",
       "  'properties': {'turns': {'type': 'array',\n",
       "    'description': '',\n",
       "    'items': {'$ref': '#/$defs/Turn'}}},\n",
       "  'title': 'Conversation',\n",
       "  'required': ['turns'],\n",
       "  '$defs': {'Turn': {'type': 'object',\n",
       "    'properties': {'msg_a': {'type': 'string', 'description': ''},\n",
       "     'msg_b': {'type': 'string', 'description': ''}},\n",
       "    'title': 'Turn',\n",
       "    'required': ['msg_a', 'msg_b']}}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = get_schema(Conversation); json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def json2proto(schema_dict):\n",
    "    \"Convert JSON schema to protobuf schema\"\n",
    "    def _convert_type(t):\n",
    "        return {'string': protos.Type.STRING, 'array': protos.Type.ARRAY, 'object': protos.Type.OBJECT}.get(t, protos.Type.TYPE_UNSPECIFIED)\n",
    "    \n",
    "    def _convert_property(prop, depth=0):\n",
    "        schema = protos.Schema(type=j2p_map.get(prop.get('type'), protos.Type.TYPE_UNSPECIFIED))\n",
    "        if 'items' in prop:\n",
    "            ref = prop['items'].get('$ref')\n",
    "            schema.items = _convert_property(schema_dict['input_schema']['$defs'][ref.split('/')[-1]], depth+1) if ref else _convert_property(prop['items'], depth+1)\n",
    "        if 'properties' in prop: schema.properties = {k: _convert_property(v, depth+1) for k,v in prop['properties'].items()}\n",
    "        if 'required' in prop: schema.required.extend(prop['required'])\n",
    "        return schema\n",
    "    \n",
    "    return _convert_property(schema_dict['input_schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227adccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_: OBJECT\n",
       "properties {\n",
       "  key: \"turns\"\n",
       "  value {\n",
       "    type_: ARRAY\n",
       "    items {\n",
       "      type_: OBJECT\n",
       "      properties {\n",
       "        key: \"msg_b\"\n",
       "        value {\n",
       "          type_: STRING\n",
       "        }\n",
       "      }\n",
       "      properties {\n",
       "        key: \"msg_a\"\n",
       "        value {\n",
       "          type_: STRING\n",
       "        }\n",
       "      }\n",
       "      required: \"msg_a\"\n",
       "      required: \"msg_b\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "required: \"turns\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Conversation schema\n",
    "proto_schema = json2proto(json_schema); proto_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2a641",
   "metadata": {},
   "source": [
    "Let's now make it into a proper tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b37434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cls2tool(c) -> genai.protos.FunctionDeclaration:\n",
    "    json_schema = get_schema(c)\n",
    "    schema = json2proto(json_schema)\n",
    "    return genai.protos.FunctionDeclaration(\n",
    "        name=json_schema['name'],\n",
    "        description=json_schema['description'],\n",
    "        parameters=schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56b482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"Conversation\"\n",
       "description: \"A conversation between two people\"\n",
       "parameters {\n",
       "  type_: OBJECT\n",
       "  properties {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      type_: ARRAY\n",
       "      items {\n",
       "        type_: OBJECT\n",
       "        properties {\n",
       "          key: \"msg_b\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        properties {\n",
       "          key: \"msg_a\"\n",
       "          value {\n",
       "            type_: STRING\n",
       "          }\n",
       "        }\n",
       "        required: \"msg_a\"\n",
       "        required: \"msg_b\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  required: \"turns\"\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_convo = cls2tool(Conversation); create_convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26648df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"create_convo\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"turns\"\n",
       "      value {\n",
       "        list_value {\n",
       "          values {\n",
       "            struct_value {\n",
       "              fields {\n",
       "                key: \"msg_b\"\n",
       "                value {\n",
       "                  string_value: \"\"\n",
       "                }\n",
       "              }\n",
       "              fields {\n",
       "                key: \"msg_a\"\n",
       "                value {\n",
       "                  string_value: \"I am sorry, I do not have access to the internet or any file systems to provide recipes.\"\n",
       "                }\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'create_convo', 'args': {'turns': [{'msg_b': '', 'msg_a': 'I am sorry, I do not have access to the internet or any file systems to provide recipes.'}]}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 109\n",
       "- candidates_token_count: 48\n",
       "- total_token_count: 157\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"create_convo\",\n",
       "                  \"args\": {\n",
       "                    \"turns\": [\n",
       "                      {\n",
       "                        \"msg_b\": \"\",\n",
       "                        \"msg_a\": \"I am sorry, I do not have access to the internet or any file systems to provide recipes.\"\n",
       "                      }\n",
       "                    ]\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 109,\n",
       "        \"candidates_token_count\": 48,\n",
       "        \"total_token_count\": 157\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gen_model.generate_content(pr, tool_config={'function_calling_config':'ANY'}); result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afcb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"create_convo\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"turns\"\n",
       "    value {\n",
       "      list_value {\n",
       "        values {\n",
       "          struct_value {\n",
       "            fields {\n",
       "              key: \"msg_b\"\n",
       "              value {\n",
       "                string_value: \"\"\n",
       "              }\n",
       "            }\n",
       "            fields {\n",
       "              key: \"msg_a\"\n",
       "              value {\n",
       "                string_value: \"I am sorry, I do not have access to the internet or any file systems to provide recipes.\"\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = contents(result).function_call; func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96499fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<proto.marshal.collections.maps.MapComposite>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [<proto.marshal.collections.maps.MapComposite object>]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = mk_args(func.args); args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133459e6",
   "metadata": {},
   "source": [
    "Let's update `mk_args` to handle nested proto objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01eb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _convert_proto(o):\n",
    "    \"Convert proto objects to Python dicts and lists\"\n",
    "    if isinstance(o, (dict,MapComposite)): return {k:_convert_proto(v) for k,v in o.items()}\n",
    "    elif isinstance(o, (list,RepeatedComposite)): return [_convert_proto(v) for v in o]\n",
    "    elif hasattr(o, 'DESCRIPTOR'): return {k.name:_convert_proto(getattr(o,k.name)) for k in o.DESCRIPTOR.fields}\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_args(args):\n",
    "    if isinstance(args, MapComposite): return _convert_proto(args)\n",
    "    return {k: v for k,v in args.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d8989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'turns': [{'msg_b': '',\n",
       "   'msg_a': 'I am sorry, I do not have access to the internet or any file systems to provide recipes.'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = mk_args(func.args); args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_tool_config(choose: list)->dict:\n",
    "    return {\"function_calling_config\": {\"mode\": \"ANY\", \"allowed_function_names\":\n",
    "    [x.__name__ if hasattr(x, '__name__') else x.name for x in choose]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs:list, # The prompt or list of prompts\n",
    "               tools:list, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    if not isinstance(tools, list): tools = [tools]\n",
    "    kwargs['tools'] = [cls2tool(x) for x in tools]\n",
    "    kwargs['tool_config'] = mk_tool_config(kwargs['tools'])\n",
    "    res = self(msgs, **kwargs)\n",
    "    ns=mk_ns(*tools)\n",
    "    parts = find_block(res).parts\n",
    "    funcs = [convert_func(p.function_call) for p in parts if hasattr(p, 'function_call')]\n",
    "    tcs = [call_func(func.name, func.inputs, ns=ns) for func in funcs]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation(turns=[{'msg_b': 'Albert, I understand your concerns. The bomb was a momentous development, one that I helped to bring about.  But I must say, I find it both thrilling and terrifying. It is a force of unimaginable power, capable of bringing about both immense destruction and unimaginable progress.  The moral questions are complex, and I am not sure I have all the answers.', 'msg_a': 'Robert, I am curious about your thoughts on the atomic bomb.  As you know, I have spoken out against its development and use.  What are your reflections on this project, its implications, and the moral questions surrounding it?'}, {'msg_b': 'That is a question I have wrestled with deeply. I believe we have a responsibility to use this power wisely, to strive for peace and understanding, even as we acknowledge the potential for great harm.  It is a heavy burden, but one we cannot shirk.', 'msg_a': 'Indeed, Robert.  I share your sense of awe and trepidation. The very notion that we can harness such destructive power is both unsettling and thought-provoking.  What do you believe our responsibility is as scientists, as humans, in the face of this new reality?'}, {'msg_b': 'I agree, Albert. We must use this moment to work towards a brighter future, a future where the forces of destruction are superseded by those of cooperation and progress.  Let us hope that our work on this bomb will not be in vain.', 'msg_a': \"Well said, Robert.  We must be vigilant in our pursuit of a world where such weapons are never used again.  Perhaps the very existence of the atomic bomb will serve as a catalyst for global cooperation and disarmament.  That would be a worthy legacy, wouldn't you agree?\"}])\n"
     ]
    }
   ],
   "source": [
    "pr = \"Create a conversation between Albert Einstein and Robert J. Oppenheimer\"\n",
    "convo = c.structured(pr, tools=[Conversation], sp=sysp)[0]; print(convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eaed74",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8b25c",
   "metadata": {},
   "source": [
    "We'll create a Chat class that will handle formatting of messages and passing along system prompts and tools, so we don't have to worry about doing that manually each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp=None, # Optional system prompt\n",
    "                 tools:Optional[list]=None,  # List of tools to make available\n",
    "                 tool_config:Optional[str]=None): # Forced tool choice\n",
    "        \"Gemini chat client.\"\n",
    "        assert model or cli\n",
    "        self.c = (cli or Client(model, sp=sp))\n",
    "        self.h,self.sp,self.tools,self.tool_config = [],sp,tools,tool_config\n",
    "\n",
    "    @property\n",
    "    def use(self): return self.c.use\n",
    "    @property\n",
    "    def cost(self): return self.c.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567fcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In: 0; Out: 0; Total: 0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"Never mention what tools you use.\"\n",
    "chat = Chat(model, sp=sp)\n",
    "chat.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _stream(self:Chat, res):\n",
    "    yield from res\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _post_pr(self:Chat, pr, prev_role):\n",
    "    if pr is None and prev_role == 'assistant':\n",
    "        raise ValueError(\"Prompt must be given after assistant completion, or use `self.cont_pr`.\")\n",
    "    if pr: self.h.append(mk_msg(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _append_pr(self:Chat,\n",
    "               pr=None,  # Prompt / message\n",
    "              ):\n",
    "    prev_role = nested_idx(self.h, -1, 'role') if self.h else 'assistant' # First message should be 'user'\n",
    "    if pr and prev_role == 'user': self() # already user request pending\n",
    "    self._post_pr(pr, prev_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(genai.GenerativeModel.generate_content)\n",
    "def __call__(self:Chat,\n",
    "             pr=None,  # Prompt / message\n",
    "             temp=0, # Temperature\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream=False, # Stream response?\n",
    "             **kwargs):\n",
    "    if isinstance(pr,str): pr = pr.strip()\n",
    "    self._append_pr(pr)\n",
    "    if self.tools: kwargs['tools'] = self.tools\n",
    "    # NOTE: Gemini specifies tool_choice via tool_config\n",
    "    if self.tool_config: kwargs['tool_config'] = mk_tool_config(self.tool_config)\n",
    "    res = self.c(self.h, stream=stream, sp=self.sp, temp=temp, maxtok=maxtok, **kwargs)\n",
    "    if stream: return self._stream(res)\n",
    "    self.h += mk_toolres(self.c.result, ns=self.tools)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778295e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Faisal! What can I do for you today? 😊 \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Nice to meet you, Faisal! What can I do for you today? 😊 \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 12\n",
       "- candidates_token_count: 16\n",
       "- total_token_count: 28\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Nice to meet you, Faisal! What can I do for you today? \\ud83d\\ude0a \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 12,\n",
       "        \"candidates_token_count\": 16,\n",
       "        \"total_token_count\": 28\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"I'm Faisal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8006a",
   "metadata": {},
   "source": [
    "Now let's make sure that context is passed properly to subsequent calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5374fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Faisal. 😊 \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Your name is Faisal. 😊 \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 38\n",
       "- candidates_token_count: 6\n",
       "- total_token_count: 44\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Your name is Faisal. \\ud83d\\ude0a \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 38,\n",
       "        \"candidates_token_count\": 6,\n",
       "        \"total_token_count\": 44\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b6819",
   "metadata": {},
   "source": [
    "We can check our uage with the `use` property. As you can see it keeps track of the history of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 50; Out: 22; Total: 72"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81175c6",
   "metadata": {},
   "source": [
    "Let's make a nice markdown representation for our docs and jupyter notebooks of our chat object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:Chat):\n",
    "    if not hasattr(self.c, 'result'): return 'No results yet'\n",
    "    last_msg = contents(self.c.result)\n",
    "    history = '\\n\\n'.join(f\"**{m['role']}**: {m['parts'][0] if isinstance(m['parts'][0],str) else m['parts'][0].text}\" \n",
    "                         for m in self.h if m['role'] in ('user','model'))\n",
    "    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n",
    "    return f\"\"\"{last_msg}\n",
    "\n",
    "<details>\n",
    "<summary>History</summary>\n",
    "\n",
    "{history}\n",
    "</details>\n",
    "{det}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d3e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Faisal. 😊 \n",
       "\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: I'm Faisal\n",
       "\n",
       "**model**: Nice to meet you, Faisal! What can I do for you today? 😊 \n",
       "\n",
       "\n",
       "**user**: What's my name?\n",
       "\n",
       "**model**: Your name is Faisal. 😊 \n",
       "\n",
       "</details>\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 50 | 0.000004 |\n",
       "| Output tokens | 22 | 0.000007 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **72** | **$0.000010** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37f240",
   "metadata": {},
   "source": [
    "Let's also make sure that streaming works correctly with the Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you, Faisal! What can I do for you today? 😊 \n"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sp)\n",
    "for o in chat(\"I'm Faisal\", stream=True):\n",
    "    o = contents(o)\n",
    "    if o and isinstance(o, str): print(o, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35d8ea",
   "metadata": {},
   "source": [
    "Let's also make sure that tool use works with the Chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a192f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 604542+6458932?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = f\"What is {a}+{b}?\"; pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Faisal! How can I help you today? \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Nice to meet you, Faisal! How can I help you today? \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 72\n",
       "- candidates_token_count: 14\n",
       "- total_token_count: 86\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Nice to meet you, Faisal! How can I help you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 72,\n",
       "        \"candidates_token_count\": 14,\n",
       "        \"total_token_count\": 86\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n",
    "chat = Chat(model, sp=sp, tools=[sums])\n",
    "r = chat(\"I'm Faisal\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"sums\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'sums', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 108\n",
       "- candidates_token_count: 29\n",
       "- total_token_count: 137\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"sums\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 108,\n",
       "        \"candidates_token_count\": 29,\n",
       "        \"total_token_count\": 137\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408b9bd",
   "metadata": {},
   "source": [
    "The model correctly calls the right function in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf082b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [name: \"sums\"\n",
       "  response {\n",
       "    fields {\n",
       "      key: \"result\"\n",
       "      value {\n",
       "        number_value: 7063474\n",
       "      }\n",
       "    }\n",
       "  }]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.h[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbbf11",
   "metadata": {},
   "source": [
    "If we inspect the history, we can see that the result of the function call has already been added. We can simply call `chat()` to pass this to the model and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f61a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The sum of 604542 and 6458932 is 7063474. \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 158\n",
       "- candidates_token_count: 29\n",
       "- total_token_count: 187\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The sum of 604542 and 6458932 is 7063474. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 158,\n",
       "        \"candidates_token_count\": 29,\n",
       "        \"total_token_count\": 187\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d1b5a",
   "metadata": {},
   "source": [
    "Now let's make sure that `tool_config` works correctly by forcing the model to pick a particular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fe00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(\n",
    "    a:int, # The number to subtract from\n",
    "    b:int # The amount to subtract\n",
    ") -> int: # Result of subtracting b from a\n",
    "    \"Returns a - b.\"\n",
    "    print(f\"Finding the diff of {a} and {b}\")\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ef89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the diff of 604542.0 and 6458932.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "function_call {\n",
       "  name: \"diff\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"b\"\n",
       "      value {\n",
       "        number_value: 6458932\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"a\"\n",
       "      value {\n",
       "        number_value: 604542\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'function_call': {'name': 'diff', 'args': {'a': 604542.0, 'b': 6458932.0}}}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 10, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 9, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 132\n",
       "- candidates_token_count: 29\n",
       "- total_token_count: 161\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"diff\",\n",
       "                  \"args\": {\n",
       "                    \"a\": 604542.0,\n",
       "                    \"b\": 6458932.0\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 132,\n",
       "        \"candidates_token_count\": 29,\n",
       "        \"total_token_count\": 161\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\"\n",
    "chat = Chat(model, sp=sp, tools=[sums, diff], tool_config=[diff])\n",
    "r = chat(f\"What is {a}+{b}?\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f1673",
   "metadata": {},
   "source": [
    "We can see that the model calls the function specified by `tool_config` even though the prompt asks for a summation, which is the expected behvior in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58508b0e",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d787d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSUNDX1BST0ZJTEUAAQEAAAxEVUNDTQJAAABtbnRyUkdCIFhZWiAH0wAEAAQAAAAAAABhY3NwTVNGVAAAAABDQU5PWjAwOQAAAAAAAAAAAAAAAAAA9tYAAQAAAADTLUNBTk8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5yVFJDAAABLAAACAxnVFJDAAABLAAACAxiVFJDAAABLAAACAxyWFlaAAAJOAAAABRnWFlaAAAJTAAAABRiWFlaAAAJYAAAABRjaGFkAAAJdAAAACxjcHJ0AAAJoAAAAEBkbW5kAAAJ4AAAAHxkbWRkAAAKXAAAAJR3dHB0AAAK8AAAABR0ZWNoAAALBAAAAAxkZXNjAAAKXAAAAJR1Y21JAAALEAAAATRjdXJ2AAAAAAAABAAAAAAEAAkADgATABgAHQAiACcALAAxADYAOwBAAEUASgBPAFQAWQBeAGMAaABtAHIAdgB7AIAAhQCKAI8AlACZAJ4AowCoAK0AsgC3ALwAwQDGAMsA0ADVANoA3wDlAOoA8AD1APsBAQEGAQwBEgEYAR4BJAErATEBNwE+AUQBSwFSAVkBXwFmAW0BdQF8AYMBigGSAZkBoQGpAbABuAHAAcgB0AHYAeEB6QHxAfoCAgILAhQCHQImAi8COAJBAkoCUwJdAmYCcAJ6AoMCjQKXAqECrAK2AsACygLVAuAC6gL1AwADCwMWAyEDLAM3A0MDTgNaA2YDcQN9A4kDlQOhA60DugPGA9MD3wPsA/kEBgQTBCAELQQ6BEcEVQRiBHAEfgSMBJoEqAS2BMQE0gThBO8E/gUNBRsFKgU5BUgFWAVnBXYFhgWVBaUFtQXFBdUF5QX1BgUGFgYmBjcGSAZYBmkGegaLBp0Grga/BtEG4wb0BwYHGAcqBzwHTwdhB3MHhgeZB6sHvgfRB+QH+AgLCB4IMghFCFkIbQiBCJUIqQi+CNII5gj7CRAJJAk5CU4JZAl5CY4JpAm5Cc8J5Qn7ChEKJwo9ClMKagqACpcKrgrFCtwK8wsKCyELOQtQC2gLgAuYC7ALyAvgC/kMEQwqDEIMWwx0DI0MpgzADNkM8g0MDSYNQA1aDXQNjg2oDcMN3Q34DhMOLg5JDmQOfw6aDrYO0Q7tDwkPJQ9BD10PeQ+WD7IPzw/sEAkQJhBDEGAQfRCbELkQ1hD0ERIRMBFOEW0RixGqEcgR5xIGEiUSRBJkEoMSoxLCEuITAhMiE0ITYxODE6QTxBPlFAYUJxRIFGkUixSsFM4U8BURFTQVVhV4FZoVvRXfFgIWJRZIFmsWjxayFtUW+RcdF0EXZReJF60X0hf2GBsYQBhlGIoYrxjUGPoZHxlFGWsZkRm3Gd0aAxoqGlAadxqeGsUa7BsTGzsbYhuKG7Eb2RwBHCkcUhx6HKMcyxz0HR0dRh1vHZkdwh3sHhYePx5pHpMevh7oHxMfPR9oH5Mfvh/pIBUgQCBsIJcgwyDvIRshSCF0IaEhzSH6IiciVCKBIq8i3CMKIzcjZSOTI8Ij8CQeJE0kfCSqJNklCCU4JWcllyXGJfYmJiZWJoYmtybnJxgnSSd5J6on3CgNKD4ocCiiKNQpBik4KWopnSnPKgIqNSpoKpsqzisBKzUraSudK9EsBSw5LG0soizXLQstQC11Last4C4WLksugS63Lu0vIy9aL5Avxy/+MDUwbDCjMNoxEjFKMYExuTHxMioyYjKbMtMzDDNFM34ztzPxNCo0ZDSeNNg1EjVMNYc1wTX8Njc2cjatNug3JDdfN5s31zgTOE84jDjIOQU5QTl+Obs5+To2OnM6sTrvOy07azupO+c8JjxlPKQ84z0iPWE9oD3gPiA+YD6gPuA/ID9hP6E/4kAjQGRApUDnQShBakGsQe5CMEJyQrRC90M6Q31DwEQDREZEikTNRRFFVUWZRd1GIkZmRqtG8Ec1R3pHv0gFSEpIkEjWSRxJYkmpSe9KNkp9SsRLC0tSS5pL4UwpTHFMuU0CTUpNkk3bTiRObU62TwBPSU+TT9xQJlBwULtRBVFQUZpR5VIwUnxSx1MSU15TqlP2VEJUjlTbVSdVdFXBVg5WW1apVvZXRFeSV+BYLlh8WMtZGlloWbdaB1pWWqVa9VtFW5Vb5Vw1XIVc1l0nXXddyV4aXmtevV8OX2BfsmAEYFdgqWD8YU9homH1Ykhim2LvY0Njl2PrZD9klGToZT1lkmXnZjxmkmbnZz1nk2fpaD9olWjsaUNpmWnwakhqn2r3a05rpmv+bFZsr20HbWBtuW4RbmtuxG8db3dv0XArcIVw33E6cZRx73JKcqVzAXNcc7h0E3RvdMx1KHWEdeF2Pnabdvh3VXezeBB4bnjMeSp5iHnnekV6pHsDe2J7wXwhfIF84H1AfaB+AX5hfsJ/I3+Ef+WARoCogQmBa4HNgi+CkYL0g1eDuYQchICE44VGhaqGDoZyhtaHOoefiASIaIjNiTOJmIn+imOKyYsvi5WL/IxijMmNMI2Xjf6OZo7NjzWPnZAFkG2Q1pE/kaeSEJJ5kuOTTJO2lCCUipT0lV6VyZYzlp6XCZd1l+CYTJi3mSOZj5n7mmia1ZtBm66cG5yJnPadZJ3SnkCerp8cn4uf+aBooNehRqG2oiWilaMFo3Wj5aRWpMalN6Wophmmi6b8p26n4KhSqMSpNqmpqhyqjqsCq3Wr6KxcrNCtRK24riyuoa8Vr4qv/7B0sOqxX7HVskuywbM3s660JLSbtRK1ibYBtni28Ldot+C4WLjRuUm5wro7urS7LbunvCG8mr0UvY++Cb6Evv6/eb/0wHDA68FnwePCX8Lbw1fD1MRRxM3FS8XIxkXGw8dBx7/IPci7yTrJuco4yrfLNsu1zDXMtc01zbXONc62zzfPuNA50LrRO9G90j/SwdND08XUSNTL1U7V0dZU1tjXW9ff2GPY59ls2fDaddr623/cBNyK3RDdlt4c3qLfKN+v4DbgveFE4cviU+La42Lj6uRz5PvlhOYN5pbnH+eo6DLovOlG6dDqWurl62/r+uyF7RDtnO4n7rPvP+/L8Fjw5PFx8f7yi/MZ86b0NPTC9VD13vZs9vv3ivgZ+Kj5N/nH+lf65/t3/Af8mP0o/bn+Sv7b/23//1hZWiAAAAAAAABvoAAAOPIAAAOPWFlaIAAAAAAAAGKWAAC3igAAGNpYWVogAAAAAAAAJKAAAA+FAAC2xHNmMzIAAAAAAAEMPwAABdz///MnAAAHkAAA/ZL///ui///9owAAA9wAAMBxdGV4dAAAAABDb3B5cmlnaHQgKGMpIDIwMDMsIENhbm9uIEluYy4gIEFsbCByaWdodHMgcmVzZXJ2ZWQuAAAAAGRlc2MAAAAAAAAAC0Nhbm9uIEluYy4AAAAAAAAAAAoAQwBhAG4AbwBuACAASQBuAGMALgAAC0Nhbm9uIEluYy4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkZXNjAAAAAAAAABNzUkdCIHYxLjMxIChDYW5vbikAAAAAAAAAABIAcwBSAEcAQgAgAHYAMQAuADMAMQAgACgAQwBhAG4AbwBuACkAABNzUkdCIHYxLjMxIChDYW5vbikAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWFlaIAAAAAAAAPbWAAEAAAAA0y1zaWcgAAAAAENSVCB1Y21JQ1NJRwAAASgBCAAAAQgAAAEAAAAAAAABAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVklUIExhYm9yYXRvcnkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAENJTkMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADzVAABAAAAARbPAAAAAAAAAAAAAAAAAAAAAwAAAAAAAAAAABQAAAAAAAEAAQAAAAAAAf/bAEMABAMDBAMDBAQDBAUEBAUGCgcGBgYGDQkKCAoPDRAQDw0PDhETGBQREhcSDg8VHBUXGRkbGxsQFB0fHRofGBobGv/bAEMBBAUFBgUGDAcHDBoRDxEaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGv/AABEIAMgBLAMBIgACEQEDEQH/xAAdAAABBAMBAQAAAAAAAAAAAAAGAwQFBwACCAEJ/8QAQhAAAgECBAQEBAMHAwQBAwUAAQIDBBEABRIhBhMxQSJRYXEHFDKBkaGxCBUjQlLB8DPR8RYkYuGCQ3KSCRg0U6L/xAAaAQADAQEBAQAAAAAAAAAAAAACAwQBBQAG/8QAMREAAgICAgEDAgQFBQEBAAAAAQIAEQMhEjEEE0FhIlEjcaHwFDKxwdFCgZHh8TNi/9oADAMBAAIRAxEAPwCtuF+JeC6SDNsygzareKiAu4kZGjuPoHS9yNididr4p74qfENuLpRSUVdVVeWU8xKySnwygjwtpYa1IuQQSRfcYr4SOheKOR9DoRKqGwbe9j54UpDTTVSxVQZKXWCxQXcLft54a3k/gjEihV+BGNlbJ2Y94WySHOc3io55DFG8bP4CNbkfyrfv+PthXiDJlyWqIiqObGygXZLFbW2I97flhtLTxQ1UlTltQwhhkC0+vZyLbHbv/vhLMcyrM2qWFdJLOyR6QWU3Fu59ffEf81EGCCApBG5GtqEpbWtyb++FquqeomvZRpGkW7+uGsWzXI2XfCmuI28LavfB0LuBZjyZ4mjjMSaSo8TEm5NulvTG9OGbTZb6+m+9/bDcVEaR20KzkX3X6fv3xYPw5pEkyvNMyraekrItaxurwGR0UeIk9lU7epsewxipyB+AT9zqexpzapC0fD1dW5LUZrDSLNSU7jVp1K+k7FwehCkgN3FwbW3xP5/8Oc3yKanWn15k8omYpTxF25UZALsPI3Htt3xa+V0wi4cC08XIQXSOmZDrcHcjTa2k6j1seuCLI6PMsypav5RJKCoI5C1OyOoB+lS4IDC43sd+nni/H4+JuCFWtxfXVdg/Yyr+HAB3OWKj5f8Af1PJVUjVUYZJKiLUVdgDYr128vTFhycHZfxrmT1OQUByOnAZOVG/MTmAdLXJB3U9dwGt0xD1fw/zKPibMstnf93yRSgCWpOtmVm+slRudwTt+eHPCua1fAMwir4m+UqQC00NjoFzbUCtydrgXBsduuIkw8GDZAaFxY+zDUY8QcD1mRSV1KZoaxcupYJ5nRSulZugt2NyL/lgZSBYAWmUBgRp0tvqOOmaeXI+IuD8zBq8rrZatGrSJZXhEukaQ8nQnSwC7nw7DyJ5eliZSyKSUDX1Hz8sZmxcFVqrluu6+LEHKoQ/TuT2U5/meUUscVBWTJSrzf4BN1OtQrgqbixsNvQHqL4e5dNV5jmaTu80swTxSu2skkWuSdybWGGL5JVZZl9DXVQpZ4KxLqsc2pkJ/wD7FG6mxv62w6yWjmirI3hvDN9cXgK+1z5nfYYk58iFY2PzhICDRhpQrkWTy/MZvTPVo8RQJKTpJPU36Bhbp64hqyVY6WSSNWSkMn/bM6/y/wBJttcd8TVCK2qhNDUPI1HGplSIMBy5CbA3PVf6u+BvMcylqqgUEhhJefSwF+XfVYEX9D1HW2MxY2V2yve9fH9qP3qWkg4wK/zH/DFRSpWPU11TyVRbRNYAFjtY37e2E+JqiD971Bgm542u2nTvbfDdYKnJ6QVFVlzrDN/28mtPDuuqyn/7WBv2JxEmllIaUROkQJ2JvYdgT9xvhysQhVj79fb/ANg0eIAkNnEoaQC/fEeV5inG+biWGtKTqUNgwB8j0whFJpRi2DXqLyG2mfLKoDNhKblqLL1xo9QSdugwizFgScMiYi9rm2HWULetX2w0OH+Si9WT5Ljx/lMwfzSXmO5HntthC76yF2uML1A7oNhvhESBH2323FsSGVDubxxCNDdmsbat74cIpN9N1Vh4Tq3OGqTeBA67A36YerOx0WW1rNYCwwlrjkqbGBjpZnKA32J3IwsaZFiR+cFJYlrE9LYSjlkcuzW3/lAtcY30c6RWIOg9rd8K3HCo8hhhsHeUyWsVBWy/c3xJ01DTz6I4njEhYF21MAl+3r74iaepMSOCU5bC7XBJ69vXDzL61OZqq1eQX1SG9t+wH5bYS11HJV7hKI5qWKalzFVEqkLHK0ZugHcE7m9xiKqKKQFV1JVSNtrAYFd/LzxI09ZV1yOtLIsskcYjsNmcX73PbCGYMaeeSB2TmI3idUA3HWx8xiZNHfcobYsdSPePTYNPA8khJBMTMQR239sNGleV3b/ti2ohjyDuQcPkrpgjRpIF+rSzDbzONI5YIkUEAORdrN1Pn6YaDUTVwNajpI6OMxpJ80d2bXZT+ONqfK2qFjpYKaRq47gmULqF+tmta3nhWlekkp+dLJNFKEvDy1DaWB737bYkqziuo4kMUmdlagwOFja+kjb02N7dMdS2F1Pn1VSNmRVPltdSVAM0UUQhY/6pDLcd+/2Iw0gqDUVUrVtS9OtrNMgLFu1j33wmk8gjsJHILWAPYb48XVbxbqDY9rjDADu4LMAKWIiAVEjLCgjDsqrv6279ziRzXhwZM1OKqVC0l9cam5QC1zfoe/4YQlmgdH0agvVreEgennho9VPVyhFMkkrOAutrkb2AwY5E66grXE33JjKuDc3zL5iopctqaqkpjaSVF2H+4tvthPJ86rch1fJTN8s0gdoyLq5B6keeCT/quu4cjqYsonJDSPZHAAjtsGF/Yf4MAkuoqJJWJdiSRbvffCsD5y7FqrVVd/NxjBFUFCb94ccPfErMslrK6qEMVRLVSiUF92TxhiinqEa1jbe3lg7rPjHxDPM9BRUyU8tZDHNVLLHZIJdZJK3uGRoyFIPXa24wC/Dykoq6SZI3nknZQainaj1x2B8LK4N0YHoTbc2sQcWFmvD4zp5uTIlJmb6Qz7KbKCOWVuPxPTttjpN5D4UVPUouaH5x2JXyAm48oKf96CjrWWiindHaDXULHNUxqvi1KSey3BJHQDDHiKiymegdc3knpIle9pA0aCS22twrWNiNjbFd55S8QcDV8/zcaU8uYQPDExUM4Q2uV8iel8GGdZ/nVbQPnlPSw09QtItDmFI6NoqYiAqBomAJlRrm/wD5LpuMFgxLjFHlaggi7Bvd/n/bUY+fkCtfpKoinVJY9UjuobZQbEjv7X/y+H1RBG8cM0c2oPcNEPqS36389seUmTs0zU9Q0lNWxy8hoOUdYbyN7WANr98aZjltZlTQx1sckLSoW8SadgSNj3Fx1GOcyNdyAKQt1qFXDtMmaUMlFT/KwzBlcTsTrBHWxHYA3097YfHK63JIvnoa5JqpY1MgdrvpdR4SQfC1j0/4AZRZjNlxL0cjJ4dDjURrB7G2Cavno80y6TM8slgy6q5apVUp25yjbw72NjbYgbEb4DHiBJrRG7v7e3/H2lSOCN+0IshzakylJRTSyzGVv4EjDSULC7E9b+La3pjQUtPV5s+Y5lO88shBVCFBA+2wP6YHChraGhShkl5hJ17BQo3vYefTfBVlmTpAEaotGFFhrJ29lG59ycT+R5RbGMa6H5To+Nj5m2Fw+yqvpqmIRSwWUWsHZSG/92GJV8sy+oheOSlQKwtp0jcehHtgOpocucCzszdjyyP7nElzkoU8FQ4QdAx/tjhZCQbud7GAdVIziv4Z0vETs9O4o6lnDGRU1a7CwB9PbAr/APt/zmojkaHNMtjRbW5sjLcfh1wcx8Scl7NJ4T3viVp+JYyLc4D0tbDcPm58QobEVm8HDlNnRgRkf7NkWYqFzHi+lpJjtpipHlA+5IGLX4b/AGGsjzQxms43rpEIuwgy+Nb+xLH9Dh5w3BNns6LEguzKVcCxBuQT9v7jHUPAXCE2XxQSvIQNjpHbzGOz4ufNlP1CcnyfHw4h9JlGj/8AT+4DNOB/1BxHzdP+pzICCfPTy/yvgEzD9gPNsrM03DvGVDWm3girKF4Sf/mrMB+GO/UgAQAja2GksOkkWuOmOmdipzABc+V3FH7OPxL4WDvWcL1NdTq1jNlzrVr+CHUPuMVfX5bWZRVy0uaUs9DUx/XBPE0br/8AFgDj7H1VExRxHcb3BHbHM/xm4SlztHpOKMoizywLU55JEqi+3LdbMnfv23xJkpBcoxr6hoGcAQFQSty3ffthzy9R1Bh0sCfLB1nXw+gpM05eVVBINx8tPKNaHuNQ+seosR388EdH8PKWoypovCk5Xa/Y9r+vUG3XED50Xdy3H42Q2KlYRUzxDmrd0XdxboMec4EFtAJFgpvi9M44fyah4cai5emELd5F2bbqb+Z6YpzMMuanqW5cfIi7ISCbHp97YSmUZLIjXwnH3GskkXiEaEra4t2J9cPqSoRI0aa2m5GkISbj2+2GkdCqxo7SOFJIZLA7X69cSFLAs/8ABvy+rqxTt064I1UFY+o6iGQPrtFbSWHiO57bG9sO66Coo2C1DKqFNeo7kL+N/wAcR0uTIoD8y0W13WPYgffD9Kaasp1pBoc72kMJBUX6YUR7iMB9o0dYZI+YFN1BGpbAknsD3wyFRBQloqhhI+okkIHtftfDk5TJH4hGXiVtLFQ2kDp+vlhSny7LzHeoZAxP88ljbA8gO5oUmVtdo4boQTv5AAXx5azKskiJyxzBcXGrsNvS2NpomlZIpCqEMQ3Tc3wznlBkeygAenXHeAufL1FZphG76Bfc2GPYYJq0yclGblrdjq7Y3jozUTFNSq2m/W32PriVyGhooczRc/ephoZFdS1LYszdgR5bG+AZgikjuEuMncgw4QaL9fxAwpSymKsQqQhS5Ut2Nuv26++Cmm4AzDMK2gFFyxR5nHLLTVMouQiMQVYDo9rbeuHj/DDNaf5yeR4Y6WOaWCmmlBvUsnWyC5CgXux2FsUMjLjLkamrjcnQgxFFDVCX595VhPiQxKC1+gvfthJ5FWkjUcmQJcKoQ3A9b4ThR1kYzMVK9QO1u/5Y2pq8RLNrQSMwKgWtf++E9dTAb0ZOcMca5pwxpipZWSheXnSQoAu5sCwI6vYGxOy3uBfFpU/xkyCrjqFq4ZaSAwQrGBDeQTHVzGVh/SdHXqPwxSLSRJyxStolt4ypP3vhCoSFHRi5IKAgAeeK8HlZMQpZoYpJvPuIDW54k+VSSRw08oliZAYwjXB1BCdKNcb6bKSLgC9sXyGkp5KesraepikqlR4pKuNA4cX8RVbqG3uO18c000EuZ1KQU6qsjtYK7BV9yT0xanDHGFdkNTl0ObzVZWlVlldKpeZMHYBSjyAqqKEA3HcnBI4IIdytkGx8H+kdhYqSalu5F8PssqpaOMNN/FqmqMzhqYlabWQTrVzvo9t97emBD4xfDhKDKIK7KJKqtahOmRjECml236bkrsSwFgCAd8WdkfEaZsMxgyHMMvnlomQVwVmlpixBOkVGlVLC3VR374kKPP5ctqWphK9PPMGGtJRddQtqXazEe2x7Y6nmv4mLHzyro0oYC+z8e1jfzK1xeqpVTOKqmOrJBaJmjsAjBbi32xKUGW1FQ2p0a/1G9gCf7DB5xvl2W8N8SSQ5fCtLTNEpCJPJL4wSG1Ft9RO59+mBw1bSAgNyl8yT+mPmPI54WOMjqFi8VG2T/tHuXzR5XpEQ507G1k/QYnYKZ6phJVVGhT11NtfyAHXA3l78ybl0zc2Ug6nIsFGJmgiiec8yQTOpAYt0Hp/6HXHLce862IUKENsuWmy+IchDJIR1ZbfliJzyYVU2ssFb+rVsPxwtWSGKApAq6EFnYnTqPrbt6YAs0zMIzFIqdbGwYktv6euI0RsjalxZca7krUVjQbGujla9rCPb8Ri0fhrw0+dIkuZVaQU7NZWWHVY+uroD/bFE5dHV5lnVNSs2uTUHcDYRj28/THZvw5kyimoIo3pI1qTHZ0dbiRfT/NsXekq0D3IfWZrI6EsTgLhlcrr4oa9EYq2kOqAAhha+wHcC/li/MvhWKNSg0g21D/y88VvwzNTPDGv8gUKl9yo7b+mLFoZwVAc72tjr+OAi6nHzsXazJiwI364bhfEQbA74wTBiL9xj1X1am7nFFycTyWNVTp2vgG4zRpqGeGljEkzLtftv54NKyQCLTqsxHXA1m0ixxGOO3TxN3/HAnYM1dGcVccZLR5RmMsUgR5w51DQNCHyB7n9PfAquaU6NpCqGHSxub++Lw+LEOWutSDMxqRYOYjpWK/RQe7Hy3vjlnMKxqbNHijYizdH3NvQ9/wAMfNeRgtjRn0vjZ/p3DqWSKZQ0/wDEC7qna/rgD4koFlnec0kxksbMguPwxMNnRESgIgIFrmQg4bGqqPrWQhP6iLr+uEYuWMx2UB5VtRFVQs6xwyAarXYbX9sbfMSEpFLBIzDoYxYdcWRVr83E9mjWYjwSgXF/XzxDCOvpgSaqIaFCguNF/uf0746mN1yDrc5OTG+M96gvFmkVPIoho5KmNTqbWLG52sLYcSZ8ZXjC0eiEbWK2J+/tgkT52UQiJ4GdDqkj0B1vfuexwuGzPW7PErwQOpSOOEXAHWxwwoD7RIYj3goM45mnnQCI72207jzPlhxTZsksZMrwxkGwVKYsAPe+CWalrDS6onjnKklLwA3U9eovcHr6YUpp6qZC1OBpBteYAMT5207DywsqK1GhjdGVBVsnPkkp/FcGwtax6YbxOI5GlYgsG/h7X3v1PoMb0TAuEtq1baffCU0dpXVbBAbCx8sdVRWp89dkmbUupqidy7FgvXuSdv1wtTsTXx8pGciRVCLcsx8u+5OPYKadKaWpjjfS7BY7kX69fx298NkpWjqDCsjRzBwLFSrBr+XUG+PLRJM9sTqulpjR0KVr5PU5Ssr8ySlqoxEUl02LCx/m7na+xth9BkUXGeVmnmqJaGCSMpL8u38TSTdkU6TYMbXI3IFvPFH8GfE3MskzPJaWeZpssjqgtXCzgmdNR3dyL7E3+2LG4m+OVPwzVtQ8Iw2rYp3p6pYtBhmUfRNGwBGrsRYjvjs48qvl9ZshCVRQgd/e5eMqcKlHcY8Pw8M8UVOXQ1lRWRBv9SWmeEkEnazgE2/qtY9sQfKEbNYi8a7m/c/4cO8xzeu4gzE1Obzz1taZSXmllaR2BJIG/YX2AsMLNkGcT0r18WW1RoF1HmiI6fD9R/ztfyOOWw5ueA1ICLOpDyPy9v6upBw8dKRsqVkEprg1lN/BywT+e+GtfSVFHUPFUxtHKrbqw3Hf77EH74cBpBBAoa0hH4Dz9AMAwqeGhNKFGhJqJNS6D4durYdTTU9TE7TNLLUki2tj0/36YcZjRxw0cMkVW05k/rNlYdyDfrfrfzx7kq5XDWqOJzUNSlG8NPIA2q23nthYaxcZxZWo+8fcM8X5xw1A9NR1WnLmqFnkp5Yg8Usi2IunUjwrtextvi3uFvjstVnGd1nE0dPAgoYjQUoVrNPGSWW9iQXBYav5TpPQYoWlkWOpVRKwphKpLutxpva5W+5t2xY+bZfSZhlE5y6OnIhd21ohDBRvc+4PmcV4/I8jFvHsf073XxHYQWB31H/xJ4nyDi/ihsx4WiqFjaCMSmawu1uw/lIuVbtdbjY2wDy6p5REr2W/iK9hjSlPIpQuoFifEb9cJCqSJySeZv0UD9cc3M7ZsrZD7zq4zxQAyVlrly2iZIBpllIVSOo9cPuHHaeeKKNdfisiD+Zj3OBqXmVb82UaV6IowXcJUzQMTFfmMpC27A9T74lygKnzHYiXf4hRnFpIXjDgQxCzHUPG3e3p+uASWCKhhqMxrCDMgIp0Jvo/8rDofLBZxRIKVWjjMemKyOzG4DW3AHc4rmtq1lPIYFkd10qLi4B3PtfE/ioWHxKfKcJDH4bZDFVVZrauVhKW1AkW38wcdEZXnlNSUxikdUbquobBv6rjp7j74qHg6SCny1UjiUSWuT2H2/5xrmVZVU78+mqVOk/Qx6/hhWTIWymHjxccQnTnA3xPWnzOGizDTypHsJAwOk/7f579I0FWskSPGwYEbEdxj5t5Hms2Yyo8EpQBrOL3KsDtjtH4WcRSvk8cFVIZGRF0k+uK/GzENwaQ+VgBXmsuZKk9jj0VvL5uo9ALYhVrBYWNziOr8yMTugbcnHQL8ZzAtyXqczLuSWNhgP4wzpqegk5J/iEeBL2JOIbPON6PK0cSzKWTqt/wGOcviL8YqivzAw0eoxXtZAT9yRiZvIC6lOPx2bftHnE0tZNK71QGrUxBeVYljHfStyTfzO574oHjl4qPM4Ji4EbHSWVgdJ8/XFhfvda6Fnaop0dhc60ZiDireN4pgrSnRMmsE6Lb/briVSGcS8gohk9SZxUrBGoaKRLeEmMEEehxMCRqmkLJIgNt1tsfQjFdZGzUyry5Q1PJuEc7XHWx7HBoQsmXvpkK3XwNqsVbt6YjyYwr6luPIWx7jLl/LAvSXCMd0PiAOEJ4KmpjAlkVvFcAoLYjcrzKSSlcOdMiNvb3xLJVc5L2VTbquGEFDFCnEio5M3oZppRUuhcgFlXYjV3/ADxvJn2ZRzNor5DqUqA52W/Ww/TD+aoHKZAWNx1ABt+eIOZEEgUFFckEEi+3kb9Dhi5GMmbGo1HT5/mtrU+ZyqYyXAY3uPK9sb5fn9bFTaDPO1mNrSCwHWw2xFyzI1lWy+I7EgEnphVNAQCRPEBvY3wXNqg8FuNOEvh/UZ/V1cM1bHl01Kf4kDqTMD2fSbXTsbG462OJ+X4Wz5bnL5dSxjMvmJeVHU6QI47gG5J2B637+WGHDnxBzk5nQ5ZFmFFQUpmHzNTNTqb+bu1ixPYb+QAxdAzWLIVoIcxmiHMk5fhXQWZuhFze58vXHdfP4qDDjyA8nYDX+L6+Zx/HxK3Jl9vvB/JsnyfhLNmkpqVa8RaFjSQhhGwO9gLg3IvfqPTEVx5ltDLllPm8VDGkyl3eo5gjMrO5MrAWLSMSfqJAVRsD1xMcSfFzhGiqa6mehqq6vSGSO1RSaBFLayrZjsL9dsV9x18UpuLCKOgp3yrKxAnMpgbI7kDbSNrBr2IsbY6OfhjGQBwVPSgdH3JPvBLoo0NiP8q+G9HUiGszVnpVqYdZUOFEZ3O3uCp9wR3wtxT8L4skjXMOE655cxjjab5XTzNMZJOot0RQndjvv6YjOFeOAyUuVZ40jUaqscDqNbBiQo1Mx2QAknY9AMHf/VnCFTV1uQtTZnXPFLKsNXTcuWJwBs5V/D6XtYW8sDjXFlHGhxI73yv3FTLRhfvKKyqr0VktTUIWVlLmQWGliO3nc4tH4VcVT1dVPkyy0NJCpvSQuHM07MT4V6rt3vbY998V5xHw9WZPFS1FcUhWvD1KjUuqxa3iVRZSL7AbHe22IWqrIdMLZaklNyxd21WJPv1xHgyN42bkB/5FK7KKudLQcNcM5rBNV5zlVPKGaNUlLxlRyyYwgYuBYEEdQCbAg7YoHjrJ4sm4praHLaLMaeBSGCV0aq9jc7aPDo8iOuHGTcfZpl2QDLFiiqKeKaQojr4THKhWWMgfUGuG67FRbBDxJwZm1TleV1uZNFl8EsCRQrKhEqgAC1u4O7XJ872OHO6+gqKCa7J739z776jD+OfpG5W7RgiRPCoAFiCDbzw2IsLyPta4BHiOJHNcsOS1BpfmIqomCKW6KRpLLq0kHuL4SoYoknY1sHOTRuWawDefXEp+m5NsEgxGgaSSri5bCLQwYH+m2+LbyLMos6o5aWk5b1TIUmhBJbSe+w327i/UXxV+YTZcrwjLoSIkjtKGa+p/MHz/AEwbfDzhueugfMFEYJIWlMMzBonUhrG2xDgFWBOsXVhtfDcIyZbVCRf/ACJVjPA0NxbPPhxmeXU09VS6ZqeIcx42dVeNbd1JvgQji0WWMBn87YuH4j8P5zXcPGukK5gYq+omu8aq1LSFysSoQAd7Eld76rgXF8VOkQjYIJAxNr+ajywHmYxgcBRQl+P69zWFSswBOuQ2A74POGEaGCon0ljDYj3tt+ZwNU9PHCTILM/0r/f/AD3wYZVMKChZqkXVAGdT/OQNhjhZ2sTo4E4mRvEFFyqaFJrtOVDG+5F9yTiv5I5JM0OgCyBQPbr/AHwf5xNLKvNqZLtJ4msd/wDgdsQWX5eZD80gBZm1FfIdsF47cFJMHyU9RgBMn4jrsgphFGn1DYhsN2h4hqaGjzGoNMkFfzGpg9UoeTQSGst79RYXG/bE5neWRT0YeVdnFiQb28jgAamkpZgJoj18LgEg+xGLfGXE68iNyHy8mfG3HlqGPC2eT5XnUYq45Iv5ZVbYkdsdefB/iiSpEEUj6WY2AJ3sB0+2OLYy8/y60+uSSKPxMbne97A+mOu/2fOHa2oSCZouQDYcyVTYjyFsS58NZQySnBlLYiHnTQreTBdbl2AAxC5xJOIaqpKtpRCdvbBeuVxx8uMDmOerW2GMzLJvm6Gtp1XRrjKhrd7dcPbGzSEMAZw7xfxW1HLU1lZUGUK1gn3/ANv1xR+d8bz5pO3LCwLfY+ntgi+KtdNTZzV0M45ZhlZGCm6kqxF/xvbFUIVFSTURtJD3CvpPvifxPGDWzS/yvK9MBVh/kmYCUamqpGfyLC34YfcQTCqyqS4R7DoU/wAOKx5hpalHy93I6lSemCCozqQUTx1IKh069be+GZPHZXBBgYvJXJjIIqPsjkhr6SWimYwzKebCx3FxsQe9j/tiehkalpmpai2iVDpcHY/7HAFlrtHNE4O4a4N7ehGC/MJWiy68hPS6nzwvOn1RuB/o/KR1GzQPUAGxuCDcH9MSUMhur7hH8uxwN0c4So8d9DrY2xOZdKYpOXKQ8T/Sw74zIvcLE3tJKoaSJOby46kdyBZgPcYiK3MoZYiQo0rbwkeJfY98Tq5TFVSMFqmpd9WpX02979cMM34KepjU0ua0yyMpJaUaAw+3fHsWMN7xWbIVvUGjVq1wQWsbg9cOFzSO3jFj5DDKs4OzyhXUhpauMk2aCpRr/a4OB+WoqaeRop0MciGzKwsRiz+GDdGQfxTL2JM0bpRzsdKz1VuY9xstug273xpm/Elfmzn56aWVVa6RhrIp8x6jDFlaOMRIAFYgykm246DGLopo5QbSGTuOg38/PDlxry5nZnLDGquKVNTJmtSavOKiWSaQqryOdbMALA372AGPZ0jjlMNJKZItdlkcabgdCfLDM6dKqthGr6iLb4d86CSnPLSzK2q7G+1+nvg2Ju4PeolGzsrLH9VrKF6Li6OAcvoc24btlFLI1dBOEq4rcxpPD9Qby6+Hp274plp2U6Y2LR27ixJ98T9JxlnOX5JFlmXZk9LSJHINMQ0XLNdmJG5Y7bnsAMNxMq8g10QRrvfz7Q8bhDc6FoOHf35TNFNHDHUwobNUwxM8SnYsI5CQNrjVY9O2Ky4c+F2UcRRzxU2dkSRGRGQpGV56E3BKn6WABVgTcXGAfiXjut4mzHLZYL5eaCnEKSRSEMT1Zi3Xc3NvXCGS8TS5PmBqMlaSBeXaXxbSDvf0vig5seNAnEvxHZOz9rjWdWayNSxKb4fZnwfNlvEVTlSfu+nQSRRfN8uZZGGzbXsF269z3wcR5PHxfSUVb8xRZlWVWuRqaStL3I8TA6RsFuL3B3NtthgVPxgoqzhmmoa7LY8wqebrqKeUMqOq/QisDfc7/bFU0PFdbkOc1dfwyf3aZQ6CNm5hRGa+m562sN8D+C2P0nFqQLF7B70dQxkGNrUy3fjbkdDT5PR1pphBmotA0sNmVmUf6bjbTYbqw28+oxRyxPydbKxRG0NJbw3sbC/n6YIzX5txGJjnc9RUzmRX0iMnchVBt2J8IB77YjHo62i1UOZmakjY89ElU2JFwGt62IvhWZ1ZrQUBQ/4+YrIOR5feROrlm7JYLuRax/8AWCrgfis8OZiJKqgWrpZotPMB0SRqpO4PQi5NwQb2FrHEdwtwhmHGOZtQ5QLSXDPLI2lFuwBJP39cOs6yWoyCuFBm0DrVGNXKF7XU/SdvPc22xgL4wHA/3gpyXYnVnDMUnEf73pKavizBYm0SCjqlSWMW2YEq1gb9R+Ixzrxhk9Nl3E1dT5fFJHHA+6TVKVDX6El0AB9rAjvviQ4Kq6Wgelkyesqcvr2IM7RzEMgB8SjYeEjbfETxVEaHPquSBzIs15dTS62a43LeVzfY9rYF/SHhrixLXE9XdA3/AL1OyjNy5N7zSGoWiBkkQuyi58h5ffC8de9W6i7FWsQL9yf8/DDKMCuURC4hcgyN3XbpiXNDFQx3iN3J2Nr6f/e/5Y4r0O+50EsjXU3r5TJGRsUTdj3Jt09gLY84KeCuplhkYcxR9N7fcHDStqUFPOFuI0j2Pc7bnBL8Nfh9V5/BrykCpIANozZ1PtfGBfwzMZvxBHVdk87ppSO8JH1FgbeuBWbIi9QUgD6na1h1OOpuG/2ceJc6p1+fqIcvgNiecSW/AYJ4v2ZY8uUClrBWVbGxlaOyRg9SB3PvheMZkF1HM+F9MZy38Ovhfm/FfFkWWZKjuSCsz6bpEvck9MfSnhDgaj4aySgy2mhT/tkUFgOpA64YfDn4e5L8OsnWHL6cCoYDnTMLvIfU4NWzSKKLVcA47GNKFt3ONlyA2qdR7S5RChDMBcY9qKWAxyDRsRY4Ypm7vuNxhRczjkVkIF/XFQZSKkvEzhX9rf4U/IStxLlVNqpmOmqEaj+H5G3ljjGSikmBYsybeW2PsD8QuH6HivIazLa1dcc8ZUjuPUHHzJ424CrOEeIK2glQskMh0MF+pL7HEjMMPUqVP4gb9pXdDlt33Jc9yB0GCKtyf5rLZpIxay2G2H9FkcjyJyhpU/zYMhl0ceXmnILeEgkixxBl8i2BEtw+MqqR95TeVIXDJKPoO4wW1qF8p5YYsbALvexHa/cYguQcuzaRGuv8TY4no5UnjaG+iRWJVSbA37fjYj74LMbIImeOKUqYOfLSQkNGbrf6T54e08zwWYn+GTuDuMLzU8ms69Ora6qb79satTGnp7uytqJ0r7G2NLX3NCcbqP8A5/VArta4cxna+2BGpq5JmbWxdb/zbj/1hStzCWVBBFFOkSuSSIzdjhkRb6Emt6xHbFWHFw2ZzPIzeoaHUVQ6bX91Hlh6cuGZBZpFZ2A03AJvbDKCIyG3LkF+gMZwZZPT1UFEFjpZmBYm/Lb/AGwxjXUnRbg7keRSZ5HXVhmgMVCmuSJ25bON9lHfy++IOvkhNY8lLEaaJyf4Y30/jhxTyOG5AvGOrINunS/nhGuax5Kr9LG58zglBDbiCw4hQIxjfRqsNX9OJGaNYHZFUgmw0gdD/h/PHtNlbTSQCVTBH1dwLg9T9sZUxVFTM/OZQNdyQ4YIOwuDbGlgT3ANRqCXflpqSykm/bD2fkk8tTKuw8ZUHfysOuF4J6Xnli3Mcm8khXZVv0H5YZTc6WtlejewUkIwIU2vtjOzBqJTQtGjKSS2m/Swt7YUgjkSByqXD6fw/wCf0wusExXmVepkUb38u3640iP/AHHhbwKuxI/O3ljb1PAyYoqRiIZEGvSx1AGxBAuF+9x+GPaTM5aLKamlkpqC9U/gmliu8dtrA22BF7jCGX5ukiVNPCjxgRG7Bt3Hc9OuFJKnLTGkU1VM3KAPLeMkBvsdziWm5UwhqShsSxuDeM8tioKKLNmppcwp4TTLNzCGeINqUNtvpIFvL0xE8XZtw3VDMp0parNM2nsiymUQ01IqiyhE+qQgd2IBJO2ACTkxzLJQGd5S2p5HVUA8gFF8MopnFTKrG3MurX9cXHNlfs6rqv1/OEXteMsD4bcUHhuurIo5IlppirytUPo0kD+Ve7EkDbtgn4r4tyTP+G8wo6OsJzPNKyOWqIhFykf0h5D/ACgAKqLt0v3JqZaCqpJkWSP/AOnrUqQdXljww1CuEglEkhvoiiIJ1ew6nA48ziwrWCK+PzHzDTIVHEiTNBWvk0zmFxPEHBkWwUNYGyg72tfBLxjWUlZQ0k2XyQSwyAC9tMkLbFkPYg32NuxwN5Nk1SKXmVcDrDIEcSEXG5ZQT5XKkWwUfuPn5eUVdW2wtcHGMSqkEbPvKsbkH4gvRiRahrX5fVvxwQwykH5Z42blLzJCdhv1Jv1xBR1L5fO8FRb+GbaCtt+1/PG81TUVkLlEM6FyQq7eHbv5g32xzXQsZ1EcKJtmTER1Ka95mGy+IgE9Pf8Azti8/wBm2FIs1eAvFJUkDl8y4VWB6MQb9O2Kmy/Jyqs00eqZU1bi9m6D774L+CKit4fzaKpp5VgAO1+wHXYYMAAVJmbkxM+j/D1BVxyyy1dVDJSMqCKJFtosN++98EvzlKPAoIANt8VXwDxhDnOSU80chYldwW322wSV1cxppGiJVyNj1GKlIURFFjuPuJMyamV2pWD2Xp2OOT/iR+0ZmNJmVRlGRx6ZYm0SGRTdWHUDzxetNUZnU0Uq5mEvuA6mxI87HHz0+Nctfl3xCzsUjySL8wShTsPbAJ+I24b/AIa6l25V+1JxRlo5dUqVA8z5YJOGP2sqmbNIqfPKcRxOQOYLGzE/pjij941aAGV3WTqwbqDjMvzKqqatEQsXZwAAL3OH+mB1JhmJ1PqrlfGdPnekxyglwCApv1wDcb/D6g4mzlJqyG9jZnXZgCOuGfwmy05Tw3lkeZShqhIVJA9r4swVKCYM4FnFr2viWua00rs4zayhsy+BNBQwyTQPJNEASAm1vwxUfEeWR5OJgqU4iS4IeVlcY6H+LnGi8IZRJMt2kkukIQkAtbv5D3xwRxZm2aZ3nM02bVMkxZtQF7IvoB0woYFJ6hHyHHvH1QP3lmUsnKVIwNgGvf1OEJE5cgLkhFsSD2t/xj2hUwROuo30jbrthCod9Cgm6s3uDgTtqEeul33PaGqMlYWa/UsB6/8AGJfK6dVro5aldcKEN9QGre4HtgfjcQSKY7Eqbn2xJPmaU0bNaQIviKKvXBV9WoBakMM/3hlg12oY21dfEOv44QGYUIksaJGXzLA/3xXTcSKSfHKP/iMJniCI3u8p91BxT6bSLmss+LMcu13+TUALpADH+2H8PEcVMnLKVK9wFk2+22Kh/wCokHR3t6xjC8XFrRrpBDAdNUQOM9Nx0IPNfvH+YZIyxHM6SmlbLlblpU6LK7X39geuG+S8FZ/xI2ZT5Dl5rUoKZ6yskDKBBEDuzaiB3Fh1PQXxJyfEqpbhaLKTEvKhj5LEWXWDftbbtgbkz/MMtppocsqZIKXMI1E6L9Mqq11BHezC/vhGE+UwYMADevfUldcXIcTY9/zjyopOTRxUEX+pI5acld7i2x7dDiEqRGlUaVHvEDpNhYXv+fvhegqqp41ijlkLytdvGdh54apTvzJJBG0rtIUiFv5u5PtcYtReN2YkCzFZI4YEaLVqDsNR67joMNtMAcrpN/U3Axvl1O1VVNFsbAlhfrbG1ZFHDM4RyXvZ1Itb1Hpgxo1c9uSlKyVVYx0gUCRaX17EIOuw7k/nbCVevMWWaJYoYyAEUAiy9gDh9kCL+78wo46KmmlrhHEJqgEGIXJ1ISRbtiCLzUcskKyuCvgIB2I329sJX6nIHtMqLZY6CoXQPEwZRbubHHk1AkSKt2kntqa5AAB3F/W3X3xmV1PJrYnpwUkVvC6WBBOHtRlpiq43NRrLESWINmF9x74I6fuejX5ZCgc6YnBF7MCD67Y8nplp21GRC7Hbfp/lxiUocupSbTF6mAy6pFhFyo3FhbvvhtXZa0tTK8dPPBSR3IMiaCQB6nrt3/8AREOLIngNXN6SloGyKqnrqqRq5njSiiA2J1eJmPYAbAe+JvIKCWCUVjRpVJEQrTIbcpidiD1Hv64EeeHQsxXl2ssYbsOgwYcA5zBTVYoqyJOdNIBENBZp3c2AZidKKBudrnFGDGS+2qNSj3LQyKvgmjaN4YZ+W4ZlkAKK4JIYj+axJNul8TUOXioqpHp4AYDa5sb6u5388a0s2TZFWsJJflpJAjI6prurXsQvcEruOu4IwX5TQTV1VJPT2RGPhChgB9juMW5ywwccrAtft9paa9oJ13A1LmlnnpIXbSQHC774iKf4crlk5lQysbWRm8QQemL4gypBEOZBd7bkLhvVZcArFFIHqMcdkhqZTMmQmIu0q9V6g7N6Yi5XOTyQT1i3ia3gA6+QA/wb+eLHzyn5EUjEaQGv6HALnTrMAropfSBta1uoA32PXCqhXLQ4F+J1LRPHbTECLNGHFz7+X546ByTiSDM6JJo5BIjC4W++PnyrPRzSVVOWjlBsFPQn1Hpi3OCPicctpk5sphlRFLsx2cn0xoBmhhOqMxroIlcq13I3A7DHJXxp4DHENfNmlDMtPIoO2/iH+XxZZ48mqqfXylm1j61bt3wL5nn89XcJQh1udmPuL4DnxMbxDCjOUhw/X1GYmmjQyPe2s3CnfrfFzfCj4S1UOcR1/ERjMVO11j6rfzJxKwpWUdass2WQhVO2hemDDK+L4aILzEMGx1bWv/bDWz2KEQuEKbMvbK62mgpFvDdR4SV6rhZ8zijktFJrv0xV1HxjREbVa2ZRffYj1wG8cfGGlyRWgyqRZ66RbJvcLbrfAqxMIyK/aNzyp4mraXL8oilPyJYzyJ5kdCv8wt3HTHOnJmp3+XnVopNyqm+x7fY2sfscWvlXED1czTSuX5jFmDG5Vj6424tycZvBDXUMIlrKRSbAfUnW9u5Hl6+mDD/6SIHH/UJVscpnki0qxJtqtsCMSstNzKN9brIVJKKvUH/P1xE5YHaARqoDk7g9rdsPpamDLJnNQ6cw78rzHb74nYEtQlqsAttIeSRYGLcxVK7EOCL+mGeY561TSGlpoVhiP1MpN2Hl6DGmZ14zOZZI1aOICyoxvbzOGBTTfHQTEKBYbnLyZTZCnUae+MwqYxc48Mdhh8nieM2xtpx5oONnpIaRUUTukICqbuwBv5WJ/P8AHDnLcvTMKeeFP9WNdaEvsV7g+WJMQz0lAlJChJ1F5YxfRqIsAfMgX3PmcRdHDTySSPG5ieM+IHZbdP16+2JA9gxdTxgsDRxU51TKB08x39cTtDl8k2UZjVxLHJFGUeSPVdi2oA7DtpJ3xF5DTPWZnBDF4+Y/L3WwYnp9vPD2qH7pzTNoKB2kpl1AKN0uW03NutiSAcLYm+IO/wDueTWzI+uZMnzOpfLomeGOdkhkfup3AtYX8JHXGAPX+J6FZJQtzyAwbqBuBe/UYmaSh/eeXKWV2Xotze0iDb8VNvtiMmj05hNFSAorI4BBvY/UB/8A5GMXIG0exPGriFJC1cTSU4lb5hty4uyW/Uf741pcqkiHNrAIyR4Vbr+HngphpTNnNDmckcdPRUa0ytBFtsE3F7WPiFzfcg4jMyRMxqaifkTjLqV0jM0y8vQWubkepBt12tjVy311NKkDUYZdLT0dY0rxLMgQqkTAr4r9fP8A5xs+YiU2dUWOTcaVuoI26H2wybMVa5SNVX+UsLk4yeWOJWBieOeNwWRhbSD/AIMM4m7MDdR+a+qjhVaVpYgw+tjb7LbYYXGfZkhjR66KWPdpFAVrqBcggjcWHfucRvMSaBkjUrKUJVgbXxGwowinYXuUAFx5sP8AbHlxg9iFQk1mtEKSqVJW0RSeKKyhV3/v0xvlmTtJVK3PSSzXPYj0OGkdRU19NFRursYFeV9TXLL9XfyF/wAcF+TUFNWT08tRI0ck0aBgADuo03b3sDc9b4IWomqplycEcPR18UEtfKlgAqknVsBt+A264vDhzhtKOl1xAyixYd29h/tirOCaSmoaRaahZpFRdK3W+/f88XbkFYlTFCkZ5aMS8hB2VFO+48zt7XwIYXLB1JjKqSCpmSnVCtQYBO8bqQUUmw1eRuCLeh8sTicMwPcTHXfcC22G3C4DXrJlMdTmzGdAw6RIAI1+yENbzc4OI4o441ubsRho+oXPAyvsz+GOX5rGQ0KKT1JF8AOffs6pmClKSvjhkLE3eG/bobdR3/vjohYRYHpjcU4fT4fGdul8AVuMBnGOefsw8WwUrnKq/La17EojM0ZHoCQfT/fFQ8Q8G8S8J/w+JcqloVmawkB1AkDzW4x9Oo6EOANCkqLb/niD4m4NyzPaCamrqKGeKQeJWS4+3374HhPanzWpOIMxy6kiipaluWCSV87HBBQfEKoEPLqYkdiNmtY9wcEXxj+Ddd8Ocw+boC1Xw/NJpilI8UBJJCP+gbv74qGrqYKe5Y6SviAv1uf/AFgeFwOREsSs+IkdNHTl0Mjuo1gHpvt+mBfNePedIGgjTRcgC/UeeAKfMXldg58RuVX+kedu2GjVS38XhA2G2PDAs96pk/VcS5hU8xmlaIOCAUHTEJqYsWd3Zw2rUd8e1dSirHHqFkjXbyv4j+uGfzYjcbgjzwxVFagFoW5HXGKULexbp74NKLM2TljVsQSfXFYZdWjX2FsGLZhEYqE8sQ3pwupe5BIJPqdr4Uy0RGK8LqSponq+ZFQU7VDHxStCL38ztcnErmfDOTZvTqaykinnYeGSwDA+wttivYs2kE4CX1J9LX3wT5PmtQtSj1R+kXsTjL49Q9NJ3JvhPwqYAtZla627CZz+pw+zL9n3hmqpXNGk+XykXVxKXX7g9sTeVV8UzozbKwve+C6gzCLMSkSozwRnc/1n19P19up+oZnpj7TnyX9m7PkpppMsqqGq135buWXw+gtsT5nt088VVnHD9bwzU1FHnFMaeuBKaG7Du336D7474nlkp1UUlpXkGy/0juxHkP8A1gT4syfhjiPKZaTiDkS7HQzgGdG/qBHiuTv5YIZANExTYx7ThlIucQFXUxNgB3Plh1yqKm8FQjzyD6ikwVQfIbG/vg/zfg6HKFlCSip5bNFHJflkx36tsSDuV2xCDKTb+BJQhO1oyfzKk40ZA/UTxMRWpqedSM6m1SjtBo3DnXpb8wfyxFTqgrJCkPIeZmdoybgDe9z5Wufvg3paWPLKNIHUu1AgeGQbbMPH03JuL/hgXo4HkmquZAUeWPxqyf6MeoaRb1IufQDzxEuUEtXQiysn+Hctgy2eN45W5xjEES6fqdyzM48gEAG/cnDJ8tApnEDLHzHEvIlkvLIm5Um3Ym7W2tcYditNJMjq55gdWF9vFa1revf74bxU8sq1ldEhKUsgildhcXZWZRfy/XEasxYuT3X7/WbftUe0WWVVHl3PiljvAqvHHf6piwvYdyRrHsoxHcU00VLn+YyjQJWqC8axpqAUtck+pLfbE3lNZT0sEEtU0o5r+M2BJN7bfiR9zjJ6J80zvI45lEazyrDOgA3B/mvba46+2MXIy5bPRB/f6frCH8shsvXTLUkoXSWYEkttZQybj8MP86zKbMJqKlm0RZfFQtBGkaD6dFjq82+mx6jSMOVrknllpVXUsRZYGX6YmW+wPmdye1ziOqcomeGolhsAUJCH+c3uRYd9hv5Y0Nb22oOwKEGcmyT5hq0tMqmlTx37ox0Ej/8AK/2w7GTQlSJpA8pDRSsFJ+k7EeewG+N6N5Y+ZNLpUSqIplk2IvffpfridysVWbZvTxrCJ6daf5Z1WTaMgncDtivJmcEmYADqCP7lmhpJM0pH5tNHVclEt412BJYdhuov03xJ8PUcdVnqU9YiwxUcySSurFlazi6A+p2Hlvg1zOAFKihy8PeprDHTwqttRBjMkt/K0dtzbfEXm1BBR1lTDBB8pTSuAwVW1am3Zwx6+3rgU8lsqfMZxrYgnRQ8yuhMkXJkaKpiqFTY6xq7edmUfbBHwtQSyVSyjUdYBNttiMP85pGNRFItOweOZQzWA1SGQ80jzDDQ33OHXDsAj5REckoVF1NYqRceW/44oTMG3PVRljcPVD0zaZiSrbEarsDvcj12/MYtPhLOY54P3VA38avqzCG86cXMjA+dtQ9CwxVyBKajSaEKJ7gJbfmKVIK2Pfp+uN+Fc2ENQMwjM38BmKBWAeO56LfysL9+uOa2cLkBH5Q7nXIqlizDJipt4pwFA2tyjt+QwRUVck0tr6iCQ3pY9Mc2T/FDMZGy96IrJPG8gZXshdSLHSN97bj/AGwdcD/EKkzNZ1RpUqElYtSuul1Gq2osdmufLHSTyUZqEMES9hKEiDqoexANz0F9yfbD6FNEms9+mA/Ls6kMsSlokD/yfUxHqen5YIIq0QOsLXZSp5XqB/L9v0xVY7hQhj6Gwxq4Gr+JutvET2GIOfiWkp6mii56K9Vuo1f0glv7Yk4qhKvY3RbizPsCfbrj3IGaIyz3g3K+JstqKLNYIqmnnj0vG4uCDjiD4zfsn55w7WVGacFpJmWStduTs0tMd77dWX1G+O95kgiURwyM7rYMBsBhJqeQrcSBbnzxoInitz49T5S+WVbU+ZI8btuQwKn0N/fGR5Oama8ahxDZjHe/MH9xb7239cfVPi/4QcLcf0zw8RZXT1EpuVnVAkqE9w43xyd8Tf2SuIcgllquCZBm1Gjao4GOmdFvf2a3pv6Yw2BYi6qcw5hlNJLUED+GJkVob7grpG1/MdO/TDROHY5IwIYrVCG1m8Sub7WB2Ht3xa78PuJ4qDO6KahmDcuOSpjaFRf/AMSLncXtYX388I1GRyZaoWRKaojY6Y5YLENcgXDX2t5G1r4h9cqtHR+Z6gRKtrMrkpauRKcEEtZYlNio8z/thzWUuZR5PEwictAsbDYjZrhgPOx0H74sA5QaymqRVRFKn6YGiQnx3UAn/wDId7/nh1X0c+WRwUqFaimhY81wPqQDQFB876yT22wtvKUATOMqzLqyqSQLUxyqQbXCkgj0O4wXUmZPBOgk2JuulhpO3n+OJaGgppal4a6jIVTpSVAHAktcM3c9vL8sO4OGqSnqIlkjZjLcLZdXLN76yb23PfyHrj2TyEA5QhYEIIc1pKem5aNKZIjyyAwuz+Snpbt+OCXI8zqqCserqlRZbmKIIA4UhRsBfT0Nrm57AYC0y2nEXOSapDvKUQJCfEbE6t9iDY9+xw9NXJRROsRKIl1c8hjrY3FtJtpboPIi2OefIJNmGOR2Ye1FfFUzSSy1s0RkVbvVFkE1xYHSvgXcEC5/PfDkZhDQUU/ycdOEmTcrHaRmAv8AVfv0ucV4lZUyyt84GSjmjaaN5CbmMGwG24G2xPcemEc5zmasjgmjjEsZ7wKbkWINztvsb9bYMZSD8zbjavyk1iiFbVCmJCpU7kFb73279fQdcRf/AExQKSJpYxIPqAHQ/fEhHWVdJHyqWROWqoC8imQqpuRa246AX6Y3k4RmzcJXmlWtNSuvXTwB1G5FjqNwdunrhwz3u6nlUt/KLg+MlzKaspXosrnp6WrXQjsmlQTcjSD9I7j0wxz/AIdrslq54pFjhnmQPUTmoVjJa2wAJIvbE09fKuQ1lUizq0VXJGp5jNrNwtgD5Db7YYcR0MkOXx5hWmCGNSlMXkBDmTRrew72Jt/xiHC5Z6J+PzP7MzgNwffLIaqMSTpK8YkvzlOmzAGxF9iL7W7DE/8Au1n4aighj5UE8pnnlTud1Uj7H88RmQZXW8YZxHT0Ec1YinxDoFQW3/zzxaL8OU8OUzVDV4OYQvyZKPQOUynbl7eSi9vM4LyXGPiCep4Y72JVVRk1UYQFheKClQa5GNkjW5Iv31E2wvlxlaphqRMY9E0cjvb6yCb2+9tsFlTwlmSsKKspqqEy2cRousvHYm4UdSoP23xlCj5jmVNk9PSGCGGVY6Zzp1Pbrq9ASSSemB9XkNwPSrUF6Vpsxr3ZKSOJKaN+UFjA5Sk/UT3JJN2a5N8N4IeRUygQ+JCHLhSXYEXAB9Cb2xalIuX5RPBSZNVJWR08jJWVkwULUSdCVW3QXsL998CCs8U8s1BFGVjLXMviuBtb79DjF8hMjsB0IRQfeRAy6PNz8tJTGeo8Ca1UhiHJ0+4vf2vgmThGlynN+ZQ5ikEdBCPAgLfPzvcOVPYLcbnbb1xIZZLLSQVGZmKNJ/BBTMtv4Tb3YeekE2HmcMmyzMFlhp85jlpI6iBXhDNpsBfRYdbXte474E5SxIQ66nqUdCQdSJJZatFmenhS8OtBcu4Ooj2vt+GHNHQZlUxsmb1NTOUkVRDpud7eYsPffD2mpUjmjoaiWOllmlI8bEsXsdz/AJ3xM0tbFV5MsDLIJxWu8gQ2ugQDqe5N/wAcYuT0koTyiRE2SfOV8FJTH5pIFMShTZnOrdRbuT0tvt5Y0i4ezGGctV060yK76YpgxNw9iTpO3Qge2JqmrpuFad8xptD1cqnlS2voU7Fh31C1gT74ypq6OBqNCJpZZ7LKUlLhh1AFrE9dybXvgUzMjfSL/e4QQGJVOXaMleSvpkhgjm061Zhp9/I39e+IekrLs9cJCBK+sEbB2Nz06Xtf8sEdFBJPR1YqCtRQSl4mjqSbuL37bXG3TuTfGlXleYvAI6GKKKAHwMyhNLbatK7A38I87C3QYU2YE1cZ6SsCRGNTLV5hT04SFQ8swkRkUtoAtp3/AA74d5BR1+VcQzVc1dJT0usgM9wpF9Qsb7sSBt5A4jKsV9NHetrH5crIWCLfcf026eWJLimWKuiybJcpgano8upRIZ2BlZ53sXkA6bXVQT0tthuF3Dd1FKt3csnKPjNJl1RK+Y86osjKgSDlqTqG4a9unbB8fi7S1KU7RySBgrTqrjQ1wuwBO2/vjm3NaaGhpkFRVaklcrZrlrC3QACxJufLriMNROmk2YqlyoUllO91P/3bdsWfxuUip467nRdb8RYv+rMiqqh1YJBWDlr9MGqO67eY39b4v/hKvqM7ghzKrVoo2UGCnYjUgP8AO/8A5ny/lHrfHz0jz98onXNZEWeKWdFZCN0jDguF7WsAMd5fC3PqXP8AIojlDwi6hwuoXIPTYYu8bIWJZupi7uWlTJBOrc5tKi1/M4kqWDL5hZ5CW6DUbYFnqRTqEaxlLWtfpfuThnnbTjLQYZhE8Uga/Zh/xi4sRsRoUNqEmZRx0stqWQsAPfEWuaxzLyqsWLXCv6+RxlJOjwQr4uxBO9x74bVtLA9PJGfAxOpST3wQY1YnqHRgN8R8gouJsqnoc0gilDKeTI6Asjdt8ct51wPTwcPxVrzkyxc1K/kR2AlQ+HbuCNj3uL747DzPk1dANbqNQKG53Djtjk74gcRQcL8S5nTzLHJk+eRGCsibqkw25g9QbH2xz/MQOoYdzVAB+qV/liyZTmMlVTzsxpFdwRcFlKGxvvtvsfNcQ5pKrMZictmgeBUfUXl0HSuonb1PX1OE1euy+orMonkCSLOqxv1uo36dwbdB54m6jMKXLq6VuZGVEoDpTIqsRe4QXHiv39fTHLChf3+/vN4Kx3oRLKcrrKbK5Mxr6eEUYqOSYFNpXlZSQBb+XbcjYb480SPK6LC6tqKusYAC2Nupsdr/AEnb9ceTZppWRrtHEZmlanZlsj2AUXPQ29euJKlqoP3FTyZdQTvU1U3N54RmZgD9NztYEfnibIxF/JqDQGhIekp+fkwjnDlZ5hqbUUKAE9N9jex8t7d9pOjkEU0kNXUTSUnMeN9D3WDUSLA+Qv5bb43aFMwzCdp0hEQIenpYruEa++oAEAb+vltbEzRZFFSU9HIuURZZDJOxmWdgjEE2Lbk9idtx5WwRyqAdw61owSqxPSzSZb+8JIXlh/iMb6Y4grAKvoQL7dz6Y0ocqkeOqaf+LRhQIJYZizFdNxc/rtc7dcEGYSwQ5j8zCqVqRFhDfYGxuAOlwLm19t8DiZj8r84s8bzQtLoVF8JUk7Hb1t1vhi5OY/OKJ1ub5ZzZamZlRkQwXgWWLSXIvqIFrg2DWHQ4VeVXctROTCbWsCtj3H1DC9DmMlHYTwgSRUspXnG5BYaVHck7nGsELZgrzxypGhYhVvptb0GNOQ9rMHKqUwgyrhT5fhxamsHOqZXqKhYzZi7uT1HYbqB7Yn6r4ZUkWS5VScXpHXGeKSplKklllLam027WFjjKd52gizCSBWSJ2cjfpawAGFOJM3rMnpqSmqJpHqIKIsjC/hVtyCepNz0x8kvk5+RYaN/ruVpxC2RG+TcB01TRR/I8SHJMoijEMc2gxMi672J6nWNX4XwU5dwpwrDSUdOnEgy+oSVpIYoaZJlle9/ET4vc27YA+G4q/Nqalr86Somy9WKllawMSjqR/nfFl5Fw5wjlnzmYzZ7U/Ps6yxCONTHbSbRnvptfYWxS2XI2Uoxuv6/5hKB9qEGsxo3OX8xnpp9VK0S1aFmKgEixHXp4rd9sCyVVHmdNrpIY0zSZuTUmyoV8NuYoHmLXtg7+VmyyCOnyiSDMDLO7J4NPhKE6WVvW2BrKaSpolzCqz6jo6aokj0RaogGFtwBbtjEyn0mLH8oLX7yHybgOCrZ8ry9poswgmEkk8Sq8UY7atRHucNOG/hnmXEccs9dmVPDBzGjREGp5LHrc2Ci+998FlFTf9M0T11VqqayvGtoCwW1rncX74hs0g4hqsyhkrlZKVo1cQUzeEI38h09PfDcXkvR47+5P9oHFANiP8+p6H4d5DFRZclKM2VedBXq4laPS1ix1bG5PYDFXCXN85RqrMJJ66sqTy2ma+y3ve/QE3Fh5A4OqqjoJ80mnqKQVJWMKgncWjIJJ8Ppt164Tr81y/MZpIjrZABGYXumr1su3UYrxZyo4qLJ2TAZS2hGua5NBmNLlWfVcQNTSQR0uZR06aQZUI0yepKgAkdTiQz+dOH6mtjyeApGJOc3hAL6twL9gAd/XDqmElDQ1B5I59VOgRQQLIfpBviL4odaeCngkliaSZwwLi6NHewF8e5M7izqeKMLgvm0lfmNFFSvqVVZqglUIF26W89u2MDylYaOkm5PzQ1zyFbWUCwt+H44XqqrNZZBTx/xam4ZUDBEsvTr0HTfEjR5bmk1HT1VTBBzJpN9EwZlUA9B33xY1KBcEKe/eDVfmlRTPFl1LJaOlbQgBsLE3LW7knfB2jR1mXUbiZVZBdUMtyttmv79sMMp4co5XqazNzXUlDLeOl5oUPI3crYDa/wDfHrvw9SF6GiyqZ6OCUtNMJW3YD+ZupOJ/IplKqN+8duqkfmNFlA0tVZnOjqNIGknmWPi0gdD64msg4aOZZfFJltbUCKna7iRLlkN7Am/QeeIWegqsudYYqeb5GqQvC918R6nf2ODfgGpi+QqcwrLKkcZLQ9OZqGnRbyAN7emFZc/DCrTFVe6kHxjldJz0yiimjjqZCJ2Ldb/0j0tcbYEsxzFIIqbJ6CNqSkTVGum7O5JuXv5E3xYVbkEmV1Gb1ahKvMKaU0lC0ig8xWXUSPKy/ngSy166CB5HCSrIOVBeK7JqJGkdyb32xR4+UuOJ9v6wSDyK9SCoIPmXaiLR7AaAEDBl7i3T1xY3wp4jzDgLM9UKyR5Xp5bMx0G99hp7Dytgcl4Sk4TGX1FSsd6tGmpY2Ymyf1E2FhfoMRuUUdRmKVFRmdTLCslQDPJqJ0xqb7eV72vipctN31ACFTTdzsHKfi9kme5mKNKuIKjAzIjX+nfY+eJ+Hi+j4kq/3UivPqWQq0Q2bbwEH1J+2OPctoaaizPmZZDG+XzORA7XB5gFrOfX88S/DPHlVwzmMVbS1TfvakqSHRt45IyALW9xis+YbqNQ8D9Qnd75O2U5fTRR+JkhVWBNze2K/wCMc4zClg0Q5fUVR1i5iW+nfe/lgX4Z+PK8Z5vlVPmdM+Q18yuqFZeZHI4W42IvvY7YMOLfinluT8N11PKIo80K2jjjN+ZY3LDvsATY46ePKmRCQdRZYg3K0zfP80onzETUk0sZPMWEGzggdTfobjFC/ECop+KP+4mppIqxAeZA+m5262NxjbPvjNmmZ8RZ1mWT1aihgVpY9dw0gvY29cCNf8QafiCGkety/XM5PNmkusiN5hh9Vu6nEvqAqR3U0nkLMc8L1Jg4lhqaipjmERKmEoBzG03PuQo7d8RmXLHmdfJDIzwATu0JnJtG5B0bHtfr5bYQoc5FRVVEaxRQpCBpqUh2Jbpde+HcDvR1QFa1PBA6iTU5Jsv/AIDf1G2OYUJJqYhBbczNIJZqtaWWop2lE7FwEsWPToB/l8Eaxx02XJlU+Yy12yycqCP/APjPY+PX0I6AgeeG1VlzU+YrDlEK1uZwohfXGV5rdRubqTY2I/HBPwZlMFQKkcTmThlw0kmkRgtI17AdbAb/AJYRk6HI/v8ArGrhHLuRJCUZpC6/K0VPUqsyQlVeRLX/ANTr59P6sFtHNS5hRmmXhv5ZGjdY6moqJZWLBbqLkgbkgdO+J9Mky+OnjrVv+6Av8NaikA5jWsNIUnYkX33v6YlKXIJsxp6isi5Jn06VVpdK6AQQLdb9N8crL5AApfaN9JhdQAreGhUU8Pzaq9BLrM0ity5EIG1xazi426e+AfM4K+Bpar5W+WxMCJyPFcbeIdh6jFomerOZ5hleZxvRoQqs0iWZjc7ggeEXO2G7w0FNVvQz1BzZ4hr5ESFF6bXc/V5Ww/DlyX9fUz0wRd1BaLh6mz7KzmdcZaqSKFdIQAc0qSNOnqdRt+Hrh5lfwyz3Nad3ossSSKGQwghiouvUAel7fbDpKmleCppqyOmVA6vy73UXNyDa3fBbPxbmEKwDKo3NO0QYBQoCnfbc4audgeI/6nsaIRvuDuQU1VPkeWZipesiWNedGn/1G+/QdMNs3zuRJKw5sYnqlVZVppfpUntfvfsMZjMcbEoNj/8AR/rNbSmNqfid86yaaCMPl8rqYoafT4bjrYeWGfDeXRnNYKWrV4wq8xWWU6mdbHceXXGYzBvSF1UV3/eK3QMIL1UuY0tZmJUxvVvbSdJTey/liTg+ezQV8TvDmlXEB8pDsoS17WY9T/tjMZiTEeQAPz/WGPqYA/vUZT8JZXDlYzriynkNaCqwrHMwMjG+zdh1wnxZmElFkyVGWTLQwpGIwIRcB7bX8xfGYzFD5GV8fya/WMYAKTBFcozSvgFdPJCalHASRTcTNYEhl9cOHkpkYGqy+qyyVZeZzob6S+5swtYX3xmMx1Vclqia4NqHeTimf5aj4lyDLqo16FmrpNTlR0Sw6AgdcDX7oqqvjGVaChSuy6BwiQyr/DAA6b9jjMZiA5W9Pl7xjfWgv7zfijhlaXLubDSRR17FkanhkusMZJNlJ/vjzgjhBjPFNl9WaymXS0qVAs6eXuL4zGYNcznAT8xNU0nuKuDnqqiPMc+zKmENECsMaMRoDdBftYfrgAzemgeHTkb1FdBTyjm06OCCtt2tYXH54zGY6XjjkpuVAAEmN6iup0y6GVowk1LZZFJIV1Gwtf07YmMgy6aeCqhgpngiiKVABa/MLHcX8wMZjMS+UePjMR7f5ElstowuzrKqWjqZq9YJ5GRkVVEhsZmILMQfww4g4eo63N+QJUpap5A2keLl2BJYKereuMxmJcBLGyZ0sONWAJ+3+YU51wcnEmVUlbn0vPkaFYKZYxoLqgIG3bpitM0ymnyujWnpagQaWK1aOba7HZScZjMMGRzkIvowcwBe4NVL0op4aCCSSIJL8wUiFgOguL+mFpqahyzPMxiy+inzCVYw8U5ewt2A9d8ZjMdGzo/EkChluMJ69oqmKeeqenrBIJUSEm6ODYW9cS2dZpnfEkbpV0Eysw0tPpAYkdx733xmMxev/wAxBCgxrnXC+T5XFllJPG8VdJTlJDGngJI/m62wCVPCkMM0EAkkid59KaASBrPU3xmMwjA7EA32P8zc+NQdSczz5HJ8zqMtSLRBAgHq3QE/554lcoypaeLLXzNZGpJQJIJta2sbkAAjqdtsZjMLZ2UKfv8A4iF25uK09WtJVVJy6okp6oOCzvclr9w98L0WcTVeZQTZvUc1I1IZFjuJOw1eZxmMxh+oG44MSKhZBnFVmdQrSCOFmAQoq6AI1H1Eeew3w/TPaSmq0aprJ68Q2VRGdIsOl/Q4zGYkPj4yLjj/ACXGPEnEnMeesmkkFRUOrTKF8N/5bdyMC1Jn9PT1RhqYFkmLMTpYgE2uLnzxmMw1MCVJ8h3ca0WWVudZhM9yqz7qWSyrvsD5YfVuWvFVzIKtp9LWJjnsAfLGYzE4as5HxPJjVhZn/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = Path('./samples/puppy.jpg')\n",
    "display.Image(filename=fn, width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc3a59",
   "metadata": {},
   "source": [
    "Now that we are passing more than just text, will need a helper function to upload media using Gemini's File API, which is the recomended way of passing media to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def media_msg(fn: Path)->dict:\n",
    "    if isinstance(fn, dict): return fn # Already processed\n",
    "    f = genai.upload_file(fn)\n",
    "    return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1316e",
   "metadata": {},
   "source": [
    "Let's also update how we pass in text type messages, to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def text_msg(s:str)->dict:\n",
    "    return {'text': s}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105009c",
   "metadata": {},
   "source": [
    "And finally lets add a helper function for make content correctly handles text and other media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed657bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _mk_content(src):\n",
    "    \"Create appropriate content data structure based on type of content\"\n",
    "    if isinstance(src,str): return text_msg(src)\n",
    "    if isinstance(src,FunctionResponse): return src\n",
    "    else: return media_msg(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5f034",
   "metadata": {},
   "source": [
    "Now let's make sure it properly handles text vs. Path objects for media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befbdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Hi'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mk_content(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ad3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_data': {'mime_type': 'image/jpeg',\n",
       "  'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/2aq3x67ccecn'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mk_content(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8705f",
   "metadata": {},
   "source": [
    "And now we need to update mk_msg to be able to handle multimedia messages correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msg(content, role='user', **kw):\n",
    "    if isinstance(content, GenerateContentResponse): role,content = 'model',contents(content)\n",
    "    if isinstance(content, dict): role,content = content['role'],content['parts']\n",
    "    if not isinstance(content, list): content=[content]\n",
    "    if role == 'user': content = [_mk_content(o) for o in content] if content else ''\n",
    "    return dict(role=role, parts=content, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_msgs(msgs:list, **kw):\n",
    "    \"Helper to set 'assistant' role on alternate messages.\"\n",
    "    if isinstance(msgs,str): msgs=[msgs]\n",
    "    return [mk_msg(o, ('user','model')[i%2], **kw) for i,o in enumerate(msgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb995eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/3vaipajdqi7v'}}]},\n",
       " {'role': 'model',\n",
       "  'parts': ['In brief, what color flowers are in this image?']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "mk_msgs([fn, q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c742c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'parts': [{'text': 'Hi'}]},\n",
       " {'role': 'model', 'parts': ['Nice, to meet you. How can I help?']},\n",
       " {'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/l8zot30312js'}},\n",
       "   {'text': 'In brief, what color flowers are in this image?'}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(['Hi', 'Nice, to meet you. How can I help?', [fn, q]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65879c1d",
   "metadata": {},
   "source": [
    "Now, we should just be able to pass a list of multimedia content to our Chat client and it should be able to handle it all under the hood. Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79448474",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061388df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers are purple. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The flowers are purple. \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 270\n",
       "- candidates_token_count: 5\n",
       "- total_token_count: 275\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The flowers are purple. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 270,\n",
       "        \"candidates_token_count\": 5,\n",
       "        \"total_token_count\": 275\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([fn, q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710a811",
   "metadata": {},
   "source": [
    "Hooray! That works, let's double check the history to make sure that everything is properly formatted and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e5016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'file_data': {'mime_type': 'image/jpeg',\n",
       "     'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/5w5f40qezxsa'}},\n",
       "   {'text': 'In brief, what color flowers are in this image?'}]},\n",
       " {'role': 'model', 'parts': ['The flowers are purple. \\n']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msgs(chat.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b810ff",
   "metadata": {},
   "source": [
    "While we are at it, let's update our markdown representation to handle the new messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _repr_markdown_(self:Chat):\n",
    "    if not hasattr(self.c, 'result'): return 'No results yet'\n",
    "    last_msg = contents(self.c.result)\n",
    "    \n",
    "    def fmt_part(ps):\n",
    "        if len(ps) == 1: return fmt_single(ps[0])\n",
    "        return '\\n' + '\\n'.join(f'- {fmt_single(p)}' for p in ps)\n",
    "        \n",
    "    def fmt_single(p):\n",
    "        if 'text' in p: return p['text']\n",
    "        if 'file_data' in p: return f\"uploaded media: {p['file_data']['mime_type']}\"\n",
    "        return str(p)\n",
    "        \n",
    "    history = '\\n\\n'.join(f\"**{m['role']}**: {fmt_part(m['parts'])}\" \n",
    "                         for m in self.h if m['role'] in ('user','model'))\n",
    "    det = self.c._repr_markdown_().split('\\n\\n')[-1]\n",
    "    return f\"\"\"{last_msg}\n",
    "\n",
    "<details>\n",
    "<summary>History</summary>\n",
    "\n",
    "{history}\n",
    "</details>\n",
    "{det}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67791706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers are purple. \n",
       "\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: \n",
       "- uploaded media: image/jpeg\n",
       "- In brief, what color flowers are in this image?\n",
       "\n",
       "**model**: The flowers are purple. \n",
       "\n",
       "</details>\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 270 | 0.000020 |\n",
       "| Output tokens | 5 | 0.000002 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **275** | **$0.000022** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133fca0",
   "metadata": {},
   "source": [
    "## Other Media (audio, video, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acd644",
   "metadata": {},
   "source": [
    "Unlike ChatGPT and Claude, Gemini models can also handle audio and video inputs. Since we're using Gemini's File API for handling multimedia content, what we have should just work, except we'll need to make one small modification to the `media_msg` function. Also, while we are at it, let us also allow for users to pass in the bytes of the content instead of the path to be consistent with our other LLM provider libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports \n",
    "def media_msg(\n",
    "    media, # Media to process (Path|bytes|dict)\n",
    "    mime=None # Optional mime type\n",
    ")->dict: # Dict for Gemini API\n",
    "    \"Handle media input as either Path or bytes, returning dict for Gemini API\"\n",
    "    if isinstance(media, dict): return media # Already processed\n",
    "    def _upload(f, mime=None):\n",
    "        f = genai.upload_file(f, mime_type=mime)\n",
    "        while f.state.name == \"PROCESSING\": time.sleep(2); f = genai.get_file(f.name)\n",
    "        return {'file_data': {'mime_type': f.mime_type, 'file_uri': f.uri}}\n",
    "    if isinstance(media, (str,Path)): return _upload(media)\n",
    "    if isinstance(media, bytes) and mime is None: mime = ft.guess(media).mime\n",
    "    return _upload(io.BytesIO(media if isinstance(media, bytes) else media.encode()), mime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40638f5e",
   "metadata": {},
   "source": [
    "Since we're uploading potentially larger files, we need to wait for the upload and process to complete so that the media is ready to be consumed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c4539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'file_data': {'mime_type': 'image/jpeg',\n",
       "   'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/x9m2m0hp8it5'}},\n",
       " {'file_data': {'mime_type': 'image/jpeg',\n",
       "   'file_uri': 'https://generativelanguage.googleapis.com/v1beta/files/zcekgao0ynim'}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_msg(fn), media_msg(fn.read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06916192",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model)\n",
    "img = fn.read_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers are purple. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The flowers are purple. \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 270\n",
       "- candidates_token_count: 5\n",
       "- total_token_count: 275\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The flowers are purple. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 270,\n",
       "        \"candidates_token_count\": 5,\n",
       "        \"total_token_count\": 275\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([img, q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc822d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll test this with the example from Gemini's docs\n",
    "video_fn = Path('./samples/selective_attention_test.mp4')\n",
    "prompt = \"Answer the question in the video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The correct answer is 15. It's easy to get distracted by the gorilla, but you need to count the passes made by the players wearing white. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"The correct answer is 15. It's easy to get distracted by the gorilla, but you need to count the passes made by the players wearing white. \\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 2, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 10527\n",
       "- candidates_token_count: 33\n",
       "- total_token_count: 10560\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The correct answer is 15. It's easy to get distracted by the gorilla, but you need to count the passes made by the players wearing white. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"LOW\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10527,\n",
       "        \"candidates_token_count\": 33,\n",
       "        \"total_token_count\": 10560\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "chat([video_fn, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e841b2",
   "metadata": {},
   "source": [
    "Takes a little while, but works like a charm! Now, let's try an audio file to make sure it also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ae3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fn = Path('./samples/attention_is_all_you_need.mp3')\n",
    "audio = audio_fn.read_bytes()\n",
    "prompt = \"What is the audio about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03856f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The audio is a podcast discussion about the research paper \"Attention is All You Need\" by Vaswani et al. The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms and does away with recurrence and convolutions. The Transformer achieved significant improvements in performance and efficiency compared to previous models, especially in machine translation tasks. The podcast discusses the model's architecture, the benefits of attention mechanisms, the results of the experiments, and the future directions of research in this field. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'The audio is a podcast discussion about the research paper \"Attention is All You Need\" by Vaswani et al. The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms and does away with recurrence and convolutions. The Transformer achieved significant improvements in performance and efficiency compared to previous models, especially in machine translation tasks. The podcast discusses the model\\'s architecture, the benefits of attention mechanisms, the results of the experiments, and the future directions of research in this field. \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 26361\n",
       "- candidates_token_count: 101\n",
       "- total_token_count: 26462\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The audio is a podcast discussion about the research paper \\\"Attention is All You Need\\\" by Vaswani et al. The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms and does away with recurrence and convolutions. The Transformer achieved significant improvements in performance and efficiency compared to previous models, especially in machine translation tasks. The podcast discusses the model's architecture, the benefits of attention mechanisms, the results of the experiments, and the future directions of research in this field. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 26361,\n",
       "        \"candidates_token_count\": 101,\n",
       "        \"total_token_count\": 26462\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([audio, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d9a18",
   "metadata": {},
   "source": [
    "Finally, let's check to make sure pdfs work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The PDF mentions the following details that were not in the previous podcast:\n",
       "\n",
       "* **Specific details about the Transformer's architecture:** The PDF provides a more detailed description of the Transformer's architecture, including the number of layers, the dimensions of the embeddings and attention heads, and the use of positional encoding.\n",
       "* **The use of label smoothing during training:** The PDF mentions the use of label smoothing, which is a technique that helps to improve the accuracy of the model by making it less confident in its predictions.\n",
       "* **Results on English constituency parsing:** The PDF discusses the Transformer's performance on the English constituency parsing task, showing that it generalizes well to other tasks besides machine translation.\n",
       "* **Examples of attention visualizations:** The PDF includes visualizations of the attention mechanisms used by the Transformer, highlighting how the model attends to different parts of the input sequence.\n",
       "\n",
       "These details provide a more in-depth understanding of the Transformer's architecture, training process, and performance across different NLP tasks. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': \"The PDF mentions the following details that were not in the previous podcast:\\n\\n* **Specific details about the Transformer's architecture:** The PDF provides a more detailed description of the Transformer's architecture, including the number of layers, the dimensions of the embeddings and attention heads, and the use of positional encoding.\\n* **The use of label smoothing during training:** The PDF mentions the use of label smoothing, which is a technique that helps to improve the accuracy of the model by making it less confident in its predictions.\\n* **Results on English constituency parsing:** The PDF discusses the Transformer's performance on the English constituency parsing task, showing that it generalizes well to other tasks besides machine translation.\\n* **Examples of attention visualizations:** The PDF includes visualizations of the attention mechanisms used by the Transformer, highlighting how the model attends to different parts of the input sequence.\\n\\nThese details provide a more in-depth understanding of the Transformer's architecture, training process, and performance across different NLP tasks. \\n\"}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 41393\n",
       "- candidates_token_count: 202\n",
       "- total_token_count: 41595\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The PDF mentions the following details that were not in the previous podcast:\\n\\n* **Specific details about the Transformer's architecture:** The PDF provides a more detailed description of the Transformer's architecture, including the number of layers, the dimensions of the embeddings and attention heads, and the use of positional encoding.\\n* **The use of label smoothing during training:** The PDF mentions the use of label smoothing, which is a technique that helps to improve the accuracy of the model by making it less confident in its predictions.\\n* **Results on English constituency parsing:** The PDF discusses the Transformer's performance on the English constituency parsing task, showing that it generalizes well to other tasks besides machine translation.\\n* **Examples of attention visualizations:** The PDF includes visualizations of the attention mechanisms used by the Transformer, highlighting how the model attends to different parts of the input sequence.\\n\\nThese details provide a more in-depth understanding of the Transformer's architecture, training process, and performance across different NLP tasks. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 41393,\n",
       "        \"candidates_token_count\": 202,\n",
       "        \"total_token_count\": 41595\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_fn = Path('./samples/attention_is_all_you_need.pdf')\n",
    "prompt = \"What's mentioned in this pdf that's not mentioned in the previous podcast?\"\n",
    "chat([pdf_fn, prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314b9b7",
   "metadata": {},
   "source": [
    "Gemini does a pretty good job here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ad692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Welcome to our podcast where we dive into groundbreaking research papers. Today we're discussing \"Attention is all you need\" by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. \n",
       "\n",
       "<details>\n",
       "\n",
       "- content: {'parts': [{'text': 'Welcome to our podcast where we dive into groundbreaking research papers. Today we\\'re discussing \"Attention is all you need\" by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I\\'m excited to discuss this revolutionary paper. Let\\'s start with the core idea. What\\'s the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. \\n'}], 'role': 'model'}\n",
       "- finish_reason: 1\n",
       "- index: 0\n",
       "- safety_ratings: [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}]\n",
       "- token_count: 0\n",
       "- grounding_attributions: []\n",
       "- avg_logprobs: 0.0\n",
       "- prompt_token_count: 41615\n",
       "- candidates_token_count: 113\n",
       "- total_token_count: 41728\n",
       "- cached_content_token_count: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Welcome to our podcast where we dive into groundbreaking research papers. Today we're discussing \\\"Attention is all you need\\\" by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 41615,\n",
       "        \"candidates_token_count\": 113,\n",
       "        \"total_token_count\": 41728\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = \"Can you generate an exact transcript of the first minute or so of the podcast.\"\n",
    "chat(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Welcome to our podcast where we dive into groundbreaking research papers. Today we're discussing \"Attention is all you need\" by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. \n",
       "\n",
       "\n",
       "<details>\n",
       "<summary>History</summary>\n",
       "\n",
       "**user**: \n",
       "- uploaded media: video/mp4\n",
       "- Answer the question in the video\n",
       "\n",
       "**model**: The correct answer is 15. It's easy to get distracted by the gorilla, but you need to count the passes made by the players wearing white. \n",
       "\n",
       "\n",
       "**user**: \n",
       "- uploaded media: audio/mpeg\n",
       "- What is the audio about?\n",
       "\n",
       "**model**: The audio is a podcast discussion about the research paper \"Attention is All You Need\" by Vaswani et al. The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms and does away with recurrence and convolutions. The Transformer achieved significant improvements in performance and efficiency compared to previous models, especially in machine translation tasks. The podcast discusses the model's architecture, the benefits of attention mechanisms, the results of the experiments, and the future directions of research in this field. \n",
       "\n",
       "\n",
       "**user**: \n",
       "- uploaded media: audio/mpeg\n",
       "- What is the audio about?\n",
       "\n",
       "**model**: The audio is a podcast discussion about the research paper \"Attention is All You Need\" by Vaswani et al. The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms and does away with recurrence and convolutions. The Transformer achieved significant improvements in performance and efficiency compared to previous models, especially in machine translation tasks. The podcast discusses the model's architecture, the benefits of attention mechanisms, the results of the experiments, and the future directions of research in this field. \n",
       "\n",
       "\n",
       "**user**: \n",
       "- uploaded media: application/pdf\n",
       "- What's mentioned in this pdf that's not mentioned in the previous podcast?\n",
       "\n",
       "**model**: The PDF mentions the following details that were not in the previous podcast:\n",
       "\n",
       "* **Specific details about the Transformer's architecture:** The PDF provides a more detailed description of the Transformer's architecture, including the number of layers, the dimensions of the embeddings and attention heads, and the use of positional encoding.\n",
       "* **The use of label smoothing during training:** The PDF mentions the use of label smoothing, which is a technique that helps to improve the accuracy of the model by making it less confident in its predictions.\n",
       "* **Results on English constituency parsing:** The PDF discusses the Transformer's performance on the English constituency parsing task, showing that it generalizes well to other tasks besides machine translation.\n",
       "* **Examples of attention visualizations:** The PDF includes visualizations of the attention mechanisms used by the Transformer, highlighting how the model attends to different parts of the input sequence.\n",
       "\n",
       "These details provide a more in-depth understanding of the Transformer's architecture, training process, and performance across different NLP tasks. \n",
       "\n",
       "\n",
       "**user**: Can you generate an exact transcript of the first minute or so of the podcast.\n",
       "\n",
       "**model**: Welcome to our podcast where we dive into groundbreaking research papers. Today we're discussing \"Attention is all you need\" by Vaswani et al. Joining us is an expert in machine learning. Welcome. Thanks for having me. I'm excited to discuss this revolutionary paper. Let's start with the core idea. What's the main thrust of this research? The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms. It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models. \n",
       "\n",
       "</details>\n",
       "\n",
       "| Metric | Count | Cost (USD) |\n",
       "|--------|------:|-----:|\n",
       "| Input tokens | 138,306 | 0.020746 |\n",
       "| Output tokens | 550 | 0.000330 |\n",
       "| Cache tokens | 0 | 0.000000 |\n",
       "| **Total** | **138,856** | **$0.021076** |"
      ],
      "text/plain": [
       "<__main__.Chat>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea161a4",
   "metadata": {},
   "source": [
    "All of these also work with `Client` and can be combined with `structured` to get structured responses using multi-media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMetadata(BasicRepr):\n",
    "    \"\"\"Class to hold metadata for audio files\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_speakers:int, # Number of speakers\n",
    "        topic:str, # Topic discussed\n",
    "        summary:str, # 100 word summary\n",
    "        transcript:list[str], # Transcript of the audio segmented by speaker\n",
    "    ): store_attr()\n",
    "pr = \"Extract the necessary information from the audio.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(model)\n",
    "audio_md = c.structured(mk_msgs([[audio_fn, pr]]), tools=[AudioMetadata])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers: 2.0\n",
      "Topic: Attention is All You Need Paper Discussion\n",
      "Summary: The paper introduces the Transformer model, which is based entirely on attention mechanisms. It eliminates recurrence and convolutions. The Transformer achieves state-of-the-art results in machine translation and other NLP tasks with significantly less training time. The paper discusses the model's architecture, attention mechanisms, and performance on various tasks. The Transformer is a significant advancement in the field of machine learning and NLP.\n",
      "Transcript: 00:00 Welcome to our podcast where we dive into groundbreaking research papers.\n",
      "-00:01 Today we’re discussing Attention is All You Need by Vaswani et al.\n",
      "-00:02 Joining us is an expert in machine learning. Welcome.\n",
      "-00:03 Thanks for having me. I’m excited to discuss this revolutionary paper.\n",
      "-00:04 Let’s start with the core idea. What’s the main thrust of this research?\n",
      "-00:05 The paper introduces a new model architecture called the Transformer, which is based entirely on attention mechanisms.\n",
      "-00:06 It completely does away with recurrence and convolutions, which were staples in previous sequence transduction models.\n",
      "-00:07 That sounds like a significant departure from previous approaches.\n",
      "-00:08 What motivated this radical change?\n",
      "-00:09 The main motivation was to address limitations in previous models, particularly the sequential nature of processing in RNNs.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of speakers: {audio_md.n_speakers}')\n",
    "print(f'Topic: {audio_md.topic}')\n",
    "print(f'Summary: {audio_md.summary}')\n",
    "transcript = '\\n-'.join(list(audio_md.transcript)[:10])\n",
    "print(f'Transcript: {transcript}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd1cd6",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850535a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
